\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{vapnik1999}
\citation{scholkopf2002}
\citation{maclagan2015}
\citation{zhang2018}
\citation{zhang2018,montufar}
\citation{maragos2020,akiangaubertqisaadi}
\citation{yoshida2019}
\citation{maragos2021}
\citation{gartner2008}
\citation{shapley1953}
\citation{gillette1957}
\citation{zwick1996}
\citation{kolokoltsov1997,gaubert2004}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation: Beyond Linear Boundaries.}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Tropical SVMs.}{1}{section.1}\protected@file@percent }
\citation{cohen2004}
\citation{AGNS10}
\citation{gaubert2011}
\citation{CuninghameGreen2003}
\citation{akiangaubertqisaadi}
\citation{Charisopoulos2017}
\citation{maragos2021}
\citation{zhang2018}
\citation{yoshida2019}
\citation{monod2022}
\citation{tang2020,Yoshida2023}
\citation{Litvinov2001,develin2004,cohen2004}
\citation{maclagan2015}
\citation{scikit-learn}
\citation{scikit-learn}
\@writefile{toc}{\contentsline {paragraph}{Background on Mean‚ÄêPayoff Games.}{2}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Contributions.}{2}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Related Work.}{2}{Item.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Left: Tropical hyperplane represented in projective space, with the constant vector $(1, 1, 1)$ projected to the origin. Each sector corresponds to a coordinate that dominates relative to the apex, here equal to $(-0.65, 0.58, 0.07)$ and located at the elbow. The two lower sectors are merged and assigned to orange points. Margin is optimal and is represented in yellow. Right: Visualization of a degree-2 polynomial classifier on a toy dataset \cite  {scikit-learn}. Each region corresponds to a sector where a specific affine combination of the features dominates, creating an interpretable piecewise-linear decision boundary.\relax }}{3}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:tropical_poly}{{1}{3}{Left: Tropical hyperplane represented in projective space, with the constant vector $(1, 1, 1)$ projected to the origin. Each sector corresponds to a coordinate that dominates relative to the apex, here equal to $(-0.65, 0.58, 0.07)$ and located at the elbow. The two lower sectors are merged and assigned to orange points. Margin is optimal and is represented in yellow. Right: Visualization of a degree-2 polynomial classifier on a toy dataset \cite {scikit-learn}. Each region corresponds to a sector where a specific affine combination of the features dominates, creating an interpretable piecewise-linear decision boundary.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Tropical Geometry Preliminaries}{3}{section.2}\protected@file@percent }
\newlabel{sec:prelim}{{2}{3}{Tropical Geometry Preliminaries}{section.2}{}}
\@writefile{toc}{\contentsline {paragraph}{The Max-Plus Semiring.}{3}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Projective Space.}{3}{equation.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Hyperplanes and Sectors.}{3}{equation.2.2}\protected@file@percent }
\citation{cohen2004}
\citation{cohen2004,develin2004}
\citation{cohen2004,AGNS10}
\citation{akiangaubertqisaadi}
\citation{cohen2004}
\citation{cohen2004}
\citation{akiangaubertqisaadi}
\citation{AGGut10}
\citation{akiangaubertqisaadi}
\citation{kolokoltsov1992}
\@writefile{toc}{\contentsline {paragraph}{Hilbert Seminorm.}{4}{equation.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Convexity and Projections.}{4}{equation.2.5}\protected@file@percent }
\newlabel{e-def-tcone}{{6}{4}{Convexity and Projections}{equation.2.6}{}}
\newlabel{e-canonical}{{7}{4}{Convexity and Projections}{equation.2.7}{}}
\newlabel{e-def-DF}{{8}{4}{Convexity and Projections}{equation.2.8}{}}
\newlabel{e-rep-fp}{{9}{4}{Convexity and Projections}{equation.2.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Spectral Framework for Tropical SVMs}{4}{section.3}\protected@file@percent }
\newlabel{sec:spectral}{{3}{4}{Spectral Framework for Tropical SVMs}{section.3}{}}
\citation{AGGut10}
\citation{nussbaum1986,AGGut10,akiangaubertqisaadi}
\citation{akiangaubertqisaadi}
\citation{gaubert2004}
\citation{akiangaubertqisaadi}
\@writefile{toc}{\contentsline {paragraph}{Shapley Operators and Their Spectral Theory.}{5}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Constructing the Classification Operator.}{5}{equation.3.10}\protected@file@percent }
\newlabel{eq:single_operator}{{11}{5}{Constructing the Classification Operator}{equation.3.11}{}}
\@writefile{toc}{\contentsline {paragraph}{Separating Point Clouds with a Hyperplane.}{5}{equation.3.11}\protected@file@percent }
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{orange}{}{orange!20}{\leavevmode {\color  {orange!20}o}}\ SG: added this observation, since later we consider separation of convex sets (rather than finite sets of points)}{5}{section*.3}\protected@file@percent }
\pgfsyspdfmark {pgfid3}{27875109}{21752041}
\pgfsyspdfmark {pgfid6}{35988437}{21680179}
\pgfsyspdfmark {pgfid7}{38101972}{21530858}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{orange}{}{orange!20}{\leavevmode {\color  {orange!20}o}}\ SG: I defined $\bar  {S}_k$ explicitely, check.}{5}{section*.4}\protected@file@percent }
\pgfsyspdfmark {pgfid8}{31932153}{19956355}
\pgfsyspdfmark {pgfid11}{35988437}{17291801}
\pgfsyspdfmark {pgfid12}{38101972}{17142480}
\@writefile{toc}{\contentsline {paragraph}{Main Theorem: Spectral Radius and Margin.}{5}{equation.3.12}\protected@file@percent }
\newlabel{thm:spectral_separability}{{1}{5}{}{theorem.1}{}}
\citation{gartner2008}
\citation{akianmfcs}
\citation{akianmfcs}
\citation{akianmfcs}
\citation{baillonbruck}
\citation{akianmfcs}
\@writefile{toc}{\contentsline {paragraph}{Benefits of the Spectral Approach.}{6}{theorem.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Limitations.}{6}{theorem.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Algorithm and Implementation}{6}{section.4}\protected@file@percent }
\newlabel{sec:algorithm}{{4}{6}{Algorithm and Implementation}{section.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Computing the Spectral Radius with Krasnoselskii--Mann Iterations.}{6}{section.4}\protected@file@percent }
\newlabel{subsec:spectral_computation}{{4}{6}{Computing the Spectral Radius with Krasnoselskii--Mann Iterations}{section.4}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Relative value iteration with Krasnoselskii--Mann damping\nobreakspace  {}\cite  {akianmfcs}\relax }}{6}{algorithm.1}\protected@file@percent }
\newlabel{alg:km_iteration}{{1}{6}{Relative value iteration with Krasnoselskii--Mann damping~\cite {akianmfcs}\relax }{algorithm.1}{}}
\@writefile{toc}{\contentsline {paragraph}{The Tropical SVM Algorithm.}{6}{ALC@unique.7}\protected@file@percent }
\newlabel{subsec:complete_algorithm}{{4}{6}{The Tropical SVM Algorithm}{ALC@unique.7}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Tropical SVM\relax }}{7}{algorithm.2}\protected@file@percent }
\newlabel{alg:tropical_svm}{{2}{7}{Tropical SVM\relax }{algorithm.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Proof of Concept.}{7}{ALC@unique.20}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Tropical Polynomials for Enhanced Expressivity}{7}{section.5}\protected@file@percent }
\newlabel{sec:polynomials}{{5}{7}{Tropical Polynomials for Enhanced Expressivity}{section.5}{}}
\@writefile{toc}{\contentsline {paragraph}{Tropical Polynomial Kernel.}{7}{section.5}\protected@file@percent }
\newlabel{e-def-feature}{{14}{7}{Tropical Polynomial Kernel}{equation.5.14}{}}
\@writefile{toc}{\contentsline {paragraph}{Strategic Monomial Selection.}{7}{equation.5.14}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Classification with Polynomials.}{8}{Item.8}\protected@file@percent }
\newlabel{subsec:poly_classification}{{5}{8}{Classification with Polynomials}{Item.8}{}}
\@writefile{toc}{\contentsline {paragraph}{Margin Guarantees.}{8}{Item.12}\protected@file@percent }
\newlabel{fig:homogeneous_selection}{{\caption@xref {fig:homogeneous_selection}{ on input line 478}}{8}{Margin Guarantees}{figure.caption.5}{}}
\newlabel{sub@fig:homogeneous_selection}{{}{8}{Margin Guarantees}{figure.caption.5}{}}
\newlabel{fig:adaptive_polynomial}{{\caption@xref {fig:adaptive_polynomial}{ on input line 483}}{8}{Margin Guarantees}{figure.caption.5}{}}
\newlabel{sub@fig:adaptive_polynomial}{{}{8}{Margin Guarantees}{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Left: Multi-class classification using a cubic polynomial classifier (degree-3 monomials). Each color represents a different class, and the boundaries show where the dominant monomial changes. Note how the polynomial naturally separates the five clusters with piecewise-linear boundaries. The light orange band around the decision boundaries indicates a lower bound on the margin, equal to $-\rho (T)/3$, as explained in the last paragraph of Section\nobreakspace  {}\ref  {sec:polynomials}. Right: Visualization of a polynomial classifier using the adaptive monomial selection strategy described in Section\nobreakspace  {}\ref  {sec:polynomials}. Rather than using all possible monomials, this approach selects terms based on pairs of points from different classes, focusing computational resources on the most discriminative features. The resulting boundary adapts closely to the data's structure while maintaining margin guarantees.\relax }}{8}{figure.caption.5}\protected@file@percent }
\newlabel{fig:poly_classifier_comparison}{{2}{8}{Left: Multi-class classification using a cubic polynomial classifier (degree-3 monomials). Each color represents a different class, and the boundaries show where the dominant monomial changes. Note how the polynomial naturally separates the five clusters with piecewise-linear boundaries. The light orange band around the decision boundaries indicates a lower bound on the margin, equal to $-\rho (T)/3$, as explained in the last paragraph of Section~\ref {sec:polynomials}. Right: Visualization of a polynomial classifier using the adaptive monomial selection strategy described in Section~\ref {sec:polynomials}. Rather than using all possible monomials, this approach selects terms based on pairs of points from different classes, focusing computational resources on the most discriminative features. The resulting boundary adapts closely to the data's structure while maintaining margin guarantees.\relax }{figure.caption.5}{}}
\citation{viro2001}
\citation{DG-06}
\citation{chaloupkathesis}
\citation{chaloupka}
\bibstyle{unsrt}
\bibdata{references}
\bibcite{vapnik1999}{{1}{}{{}}{{}}}
\bibcite{scholkopf2002}{{2}{}{{}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Connection to Classical SVMs}{9}{section.6}\protected@file@percent }
\newlabel{sec:maslov}{{6}{9}{Connection to Classical SVMs}{section.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Comparison of hyperplanes obtained through Maslov dequantization and our spectral approach. The limiting hyperplane (red) from dequantization is suboptimal compared to the spectral classifier (black).\relax }}{9}{figure.caption.6}\protected@file@percent }
\newlabel{fig:maslov_dequantization}{{3}{9}{Comparison of hyperplanes obtained through Maslov dequantization and our spectral approach. The limiting hyperplane (red) from dequantization is suboptimal compared to the spectral classifier (black).\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Discussion and Future Work}{9}{section.7}\protected@file@percent }
\newlabel{sec:discussion}{{7}{9}{Discussion and Future Work}{section.7}{}}
\bibcite{maclagan2015}{{3}{}{{}}{{}}}
\bibcite{zhang2018}{{4}{}{{}}{{}}}
\bibcite{montufar}{{5}{}{{}}{{}}}
\bibcite{maragos2020}{{6}{}{{}}{{}}}
\bibcite{akiangaubertqisaadi}{{7}{}{{}}{{}}}
\bibcite{yoshida2019}{{8}{}{{}}{{}}}
\bibcite{maragos2021}{{9}{}{{}}{{}}}
\bibcite{gartner2008}{{10}{}{{}}{{}}}
\bibcite{shapley1953}{{11}{}{{}}{{}}}
\bibcite{gillette1957}{{12}{}{{}}{{}}}
\bibcite{zwick1996}{{13}{}{{}}{{}}}
\bibcite{kolokoltsov1997}{{14}{}{{}}{{}}}
\bibcite{gaubert2004}{{15}{}{{}}{{}}}
\bibcite{cohen2004}{{16}{}{{}}{{}}}
\bibcite{AGNS10}{{17}{}{{}}{{}}}
\bibcite{gaubert2011}{{18}{}{{}}{{}}}
\bibcite{CuninghameGreen2003}{{19}{}{{}}{{}}}
\bibcite{Charisopoulos2017}{{20}{}{{}}{{}}}
\bibcite{monod2022}{{21}{}{{}}{{}}}
\bibcite{tang2020}{{22}{}{{}}{{}}}
\bibcite{Yoshida2023}{{23}{}{{}}{{}}}
\bibcite{Litvinov2001}{{24}{}{{}}{{}}}
\bibcite{develin2004}{{25}{}{{}}{{}}}
\bibcite{scikit-learn}{{26}{}{{}}{{}}}
\bibcite{AGGut10}{{27}{}{{}}{{}}}
\bibcite{kolokoltsov1992}{{28}{}{{}}{{}}}
\bibcite{nussbaum1986}{{29}{}{{}}{{}}}
\bibcite{akianmfcs}{{30}{}{{}}{{}}}
\bibcite{baillonbruck}{{31}{}{{}}{{}}}
\bibcite{viro2001}{{32}{}{{}}{{}}}
\bibcite{DG-06}{{33}{}{{}}{{}}}
\bibcite{chaloupkathesis}{{34}{}{{}}{{}}}
\bibcite{chaloupka}{{35}{}{{}}{{}}}
\bibcite{allamigeon_condition}{{36}{}{{}}{{}}}
\bibcite{harris2020array}{{37}{}{{}}{{}}}
\bibcite{MNIST}{{38}{}{{}}{{}}}
\citation{akiangaubertqisaadi}
\citation{AGGut10}
\citation{gaubert2004}
\@writefile{toc}{\contentsline {section}{\numberline {A}Proof of Theorem\nobreakspace  {}\ref  {thm:spectral_separability} (Spectral Radius and Margin)}{12}{appendix.A}\protected@file@percent }
\newlabel{appendix:proofs}{{A}{12}{Proof of Theorem~\ref {thm:spectral_separability} (Spectral Radius and Margin)}{appendix.A}{}}
\newlabel{lemma:hyperplane_to_operator}{{2}{12}{Upper bound on margin}{theorem.2}{}}
\newlabel{e-toreverse}{{17}{12}{Proof of Theorem~\ref {thm:spectral_separability} (Spectral Radius and Margin)}{equation.A.17}{}}
\newlabel{e-ourkey}{{18}{12}{Proof of Theorem~\ref {thm:spectral_separability} (Spectral Radius and Margin)}{equation.A.18}{}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{orange}{}{orange!20}{\leavevmode {\color  {orange!20}o}}\ SG: Theo, your argument was ok but a bit fast, but I have added details}{12}{section*.7}\protected@file@percent }
\pgfsyspdfmark {pgfid16}{30447488}{18910004}
\pgfsyspdfmark {pgfid19}{35988437}{18838142}
\pgfsyspdfmark {pgfid20}{38101972}{18688821}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{orange}{}{orange!20}{\leavevmode {\color  {orange!20}o}}\ SG: I have added this discussion to explain and motivate the generality}{12}{section*.8}\protected@file@percent }
\pgfsyspdfmark {pgfid21}{26438658}{10255156}
\pgfsyspdfmark {pgfid24}{35988437}{10183294}
\pgfsyspdfmark {pgfid25}{38101972}{10033973}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{orange}{}{red!50}{\leavevmode {\color  {red!50}o}}\ I need range of $T$ to be bounded in Hilbert seminorm for the proof of the following lemma. It is not obvious in general that there is a {\em  finite} vector $a$ such $T(a)=\rho +a$.}{12}{section*.9}\protected@file@percent }
\pgfsyspdfmark {pgfid26}{20112833}{8715608}
\newlabel{lemma:operator_to_hyperplane}{{3}{12}{Max-margin hyperplane}{theorem.3}{}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{orange}{}{orange!20}{\leavevmode {\color  {orange!20}o}}\ SG: caveat new condition}{12}{section*.10}\protected@file@percent }
\pgfsyspdfmark {pgfid27}{12174693}{5738821}
\pgfsyspdfmark {pgfid30}{35988437}{5671386}
\pgfsyspdfmark {pgfid31}{38101972}{5522065}
\citation{allamigeon_condition}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{orange}{}{orange!20}{\leavevmode {\color  {orange!20}o}}\ SG: existence proof added}{13}{section*.11}\protected@file@percent }
\pgfsyspdfmark {pgfid32}{16595439}{44058214}
\pgfsyspdfmark {pgfid35}{35988437}{43986352}
\pgfsyspdfmark {pgfid36}{38101972}{43837031}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{orange}{}{orange!20}{\leavevmode {\color  {orange!20}o}}\ SG: added explanation}{13}{section*.12}\protected@file@percent }
\pgfsyspdfmark {pgfid37}{17462628}{32328461}
\pgfsyspdfmark {pgfid40}{35988437}{32256599}
\pgfsyspdfmark {pgfid41}{38101972}{32107278}
\newlabel{lemma:perturbation}{{4}{13}{Overlap interpretation in the binary classification problem}{theorem.4}{}}
\citation{akiangaubertqisaadi}
\citation{allamigeon_condition}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Non-separable case: $\rho (T) \geqslant 0$, indicating class overlap. The spectral radius quantifies the minimum perturbation required to achieve separability, and the inner radius of the convex hulls' intersection.\relax }}{14}{figure.caption.13}\protected@file@percent }
\newlabel{fig:non_separable}{{4}{14}{Non-separable case: $\rho (T) \geq 0$, indicating class overlap. The spectral radius quantifies the minimum perturbation required to achieve separability, and the inner radius of the convex hulls' intersection.\relax }{figure.caption.13}{}}
\newlabel{eq:major}{{26}{14}{Proof of Theorem~\ref {thm:spectral_separability} (Spectral Radius and Margin)}{equation.A.26}{}}
\newlabel{eq:rewrite}{{27}{14}{Proof of Theorem~\ref {thm:spectral_separability} (Spectral Radius and Margin)}{equation.A.27}{}}
\newlabel{eq:dist}{{28}{14}{Proof of Theorem~\ref {thm:spectral_separability} (Spectral Radius and Margin)}{equation.A.28}{}}
\citation{scikit-learn}
\citation{harris2020array}
\citation{MNIST}
\citation{MNIST}
\@writefile{toc}{\contentsline {section}{\numberline {B}Proof of Concept}{15}{appendix.B}\protected@file@percent }
\newlabel{appendix:empirical}{{B}{15}{Proof of Concept}{appendix.B}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Performance comparison across benchmark datasets (5-fold cross-validation). Accuracy standard deviations range from 1.1\% to 9.7\%. \#C: number of classes; \#S: number of samples; \#KM: average number of Krasnoselskii--Mann iterations; $\rho (T)$: spectral radius. Convergence threshold $\varepsilon = 10^{-12}$.\relax }}{15}{table.caption.14}\protected@file@percent }
\newlabel{tab:benchmark_results}{{1}{15}{Performance comparison across benchmark datasets (5-fold cross-validation). Accuracy standard deviations range from 1.1\% to 9.7\%. \#C: number of classes; \#S: number of samples; \#KM: average number of Krasnoselskii--Mann iterations; $\rho (T)$: spectral radius. Convergence threshold $\varepsilon = 10^{-12}$.\relax }{table.caption.14}{}}
\newlabel{fig:pca_degree_scaling}{{5a}{16}{Training time vs. number of monomials\relax }{figure.caption.15}{}}
\newlabel{sub@fig:pca_degree_scaling}{{a}{16}{Training time vs. number of monomials\relax }{figure.caption.15}{}}
\newlabel{fig:sample_size_scaling}{{5b}{16}{Training time vs. size of training set\relax }{figure.caption.15}{}}
\newlabel{sub@fig:sample_size_scaling}{{b}{16}{Training time vs. size of training set\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Performance scaling analysis of tropical SVM on MNIST data \cite  {MNIST}.\relax }}{16}{figure.caption.15}\protected@file@percent }
\newlabel{fig:scaling_analysis}{{5}{16}{Performance scaling analysis of tropical SVM on MNIST data \cite {MNIST}.\relax }{figure.caption.15}{}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\gdef \@abspage@last{22}
