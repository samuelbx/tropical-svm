\documentclass[oneside,english,a4paper]{amsart}
\usepackage{bbold}
\usepackage[T1]{fontenc}
\usepackage{inputenc}
\setlength{\parskip}{\smallskipamount}
\setlength{\parindent}{0pt}
\usepackage{amstext}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{pgfplots}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{wasysym}
\usepackage{fullpage}

\makeatletter
\numberwithin{equation}{section}
\numberwithin{figure}{section}
\theoremstyle{plain}
\newtheorem{thm}{\protect\theoremname}
\theoremstyle{definition}
\newtheorem{defn}[thm]{\protect\definitionname}
\theoremstyle{plain}
\newtheorem{prop}[thm]{\protect\propositionname}
\theoremstyle{remark}
\newtheorem{rem}[thm]{\protect\remarkname}
\theoremstyle{plain}
\newtheorem{lem}[thm]{\protect\lemmaname}
\theoremstyle{definition}
\newtheorem{example}[thm]{\protect\examplename}
\newtheorem{cor}[thm]{\protect\corollaryname}
\theoremstyle{definition}

\makeatother

\usepackage{babel}
\providecommand{\definitionname}{Definition}
\providecommand{\lemmaname}{Lemma}
\providecommand{\propositionname}{Proposition}
\providecommand{\remarkname}{Remark}
\providecommand{\theoremname}{Theorem}
\providecommand{\examplename}{Example}
\providecommand{\corollaryname}{Corollary}

\begin{document}
\title{Tropical Support Vector Machines and mean payoff games}
\author{Xavier Allamigeon, Stéphane Gaubert, Samuel Boïté, Théo Molfessis}
\maketitle

\begin{abstract}
    We develop new algorithms around tropical support vector machines (SVMs) for binary and multi-classification. We explore the use of tropical hyperplanes to partition tropical space, laying the groundwork for our classification approach. We address the challenge of separating overlapping data in tropical space, proposing a method to measure and handle data overlap using non-expansive Shapley operators and a Krasnoselskii-Mann iteration scheme. For separable data, we extend our method to determine hard-margin tropical hyperplanes, ensuring maximal separation. This is further applied to multi-classification scenarios, providing conditions for tropical separability and demonstrating margin optimality. Additionally, the tropical kernel trick is explored as a means to embed data into a higher-dimensional tropical space, transforming decision boundaries into tropical polynomials. This approach is empirically tested on several classic datasets. Finally, we show that tropical hyperplanes emerge as limiting cases of classical hyperplanes on logarithmic paper, making our approach more stable numerically.
\end{abstract}

\section{Introduction}

\subsection*{The tropical linear classification problem}

% TODO: add an introduction to explain the relevance of the topic

The tropical \emph{max-plus} semifield $\mathbb{R}_{\max}$ is the
set of real numbers, completed by $-\infty$ and equipped with the
addition $a\oplus b=\max(a,b)$ and the multiplication $a\odot b=a+b$.
A \emph{tropical hyperplane} in the $d$-dimensional tropical vector
space $\mathbb{R}_{\max}^{d}$ is a set of vectors of the form
\[
\mathcal{H}_{a}=\left\{x\in\mathbb{R}_{\max}^{d},\quad\max_{1\le i\le d}a_{i}+x_{i}\,\text{is achieved at least twice}\right\}.
\]
Such a hyperplane is parametrized by the vector $a=(a_{1},\ldots,a_{d})\in\mathbb{R}_{\max}^{d}$,
which is required to be non-identically $-\infty$. It hence partitions
the space between $d$ different \emph{tropical sectors}, laying the
ground for multi-class classification.

In this paper, we address the following tropical analogue of support
vector machines. Given $n\le d$ classes of $d$-dimensional data
points $X^{1},\ldots,X^{p}$, we look for a maximum-margin separating
tropical hyperplane to build a classifier. The notion of margin depends
on the metric: the canonical choice in tropical geometry is the (additive
version of) Hilbert's projective metric. Its restriction to $\mathbb{R}^{d}$
is induced by the so-called \emph{Hilbert's seminorm} or \emph{Hopf
oscillation}
\[
\lVert x\rVert_{H}:=\max_{i\in[d]}x_{i}-\min_{i\in[d]}x_{i}.
\]
It is a projective metric, in the sense that the distance between
two points is zero if and only if these two points differ by an additive
constant. When dealing with hard-margin support vector machines (SVMs),
we also have to ensure every point lands in the appropriate sector.
We define the \emph{tropical parametrized hyperplane} of configuration
$\sigma=\left\{I^{1},\ldots,I^{n}\right\}$, where $I^{k}$ and $I^{\ell}$
are disjoint for $k\ne\ell$, as:
\[
\mathcal{H}_{a}^{\sigma}=\left\{x\in\mathbb{R}_{\max}^{d},\quad\exists k\ne\ell,\quad(x-a)\,\text{reaches its max coordinate in \ensuremath{I^{k}} and \ensuremath{I^{\ell}}}\right\}.
\]

Hence, $\mathcal{H}_{a}^{\sigma}$ is said to \emph{separate} point
clouds $(X^{k})_{k\in[p]}$ with a margin $\nu$ when for every point
$x^{k}$ of class $k$:
\begin{enumerate}
\item $x^{k}$ is on the right sector of the hyperplane, i.e its maximum
coordinate is achieved in $I^{k}$
\item $x^{k}$ is at distance at least $\nu$ from $\mathcal{H}_{a}^{\sigma}$:
\begin{equation}
d(\mathcal{H}_{a}^{\sigma},x^{k})=\max(x^{k}-a)-\max_{\sqcup_{\ell\ne k}I^{\ell}}(x^{k}-a)\ge\nu.
\end{equation}
\end{enumerate}
In Figure \ref{MaxMarginHyperplane}, we separate two classes
of $3$-dimensional points from a toy tropical dataset using a tropical
hyperplane. Its margin is maximal and is represented by the Hilbert
ball in gray. The figure is represented in the projective space $\mathbb{P}\left(\mathbb{R}_{\text{max}}^{3}\right)$,
i.e. the quotient of the set of non-identically $-\infty$ vectors
of $\mathbb{R}_{\text{max}}^{3}$ by the equivalence relation which
identifies tropically proportional values.

\begin{figure}
\input{fig/tropical_svm.pgf}\label{MaxMarginHyperplane}

\caption{Maximum-margin tropical parametrized hyperplane, binary setting}
\end{figure}


\subsection*{Results}

TBD

\section{Preliminaries on mean-field games}

In this section, we have a finite set of points $X=(x_{1},\ldots,x_{p})\in\mathbb{R}_{\text{max}}^{d\times p}$.
We define the \emph{tropical span} of $X$ as the set of tropical
linear combinations of these points:
\[
\text{Span}(X):=\left\{\max_{1\le i\le p}(x_{i}+\lambda_{i}),\quad\lambda\in\mathbb{R}^{p}\right\}.
\]
As this is also the smallest tropically convex set containing these
points, separating several point clouds amounts to separating their
tropical spans.

Given a tropically convex, compact and nonempty subset $\mathcal{V}$
of $\mathbb{R}_{\max}^{d}$, we define the projection of any vector
$x$ in $\mathbb{R}_{\max}^{d}$ by:
\[
P_{V}(x):=\max\{y\in V,\quad y\le x\}.
\]
When $\mathcal{V}$ is the tropical span of $X$, we will also note
this projection as $P_{X}$.

We know from Maclagan et al. \cite{Maclagan2015} that tropical projections
can be expressed as a mean payoff game: for $i\in[p]$ and $x\in\mathbb{R}_{\max}^{d}$,
\begin{equation}
P_{X}(x)_{i}=\max_{j\in[p]}\left\{X_{ij}+\min_{k\in[d]}(-X_{kj}+x_{k})\right\}.\label{eq:projector}
\end{equation}

Here, we recognize the payoff a zero-sum deterministic game with perfect
information. There are two players, ``Max'' and ``Min'' (the maximizer
and the minimizer), who alternate their actions. Starting on node
$i$, Player Max chooses to move to node $j$, and recieves $X_{ij}$
from Player Min. Similarily, Player Min in turn chooses a node $k$
and has to pay $-X_{kj}$ to Player Max.

We now recall some tools from Perron-Frobenius theory, in relation
with mean payoff games. We refer the reader to \cite{AKIAN2012} for more
information.

We call \emph{Shapley operator} a map $T:\mathbb{R}_{\max}^{d}\rightarrow\mathbb{R}_{\max}^{d}$
that is order preserving, continuous, and such that $T(\alpha+x)=\alpha+T(x)$
for all $\alpha\in\mathbb{R}_{\max}$ and $x\in\mathbb{R}_{\max}^{d}$.
Observe that the tropical projection defined by \ref{eq:projector}
is a Shapley operator. Given a Shapley operator $T$, we also define
\[
\mathcal{S}(T):=\left\{x\in\mathbb{R}_{\max}^{d},\quad x\le T(x)\right\}.
\]
More specifically, any tropically convex set $\mathcal{V}$ is equal
to $\mathcal{S}(P_{\mathcal{V}})$. Indeed, as for any $x\in\mathbb{R}_{\text{max}}^{d}$,
$P_{\mathcal{V}}(x)\le x$, having $x\in\mathcal{S}(P_{\mathcal{V}})$
is sufficient to have $x=P_{\mathcal{V}}(x)$, meaning $x$ is in
$\mathcal{V}$.

\section{Hard-margin binary SVM}

In this section, we suppose that a separating tropical hyperplane
exists, and we want to maximize the margin. We also place ourselves
in the binary case, where we seek to separate convex hulls $\mathcal{V}^{+}$
and $\mathcal{V}^{-}$. We assume we have two Shapley operators $T^{+}$
and $T^{-}$ such that $\mathcal{S}(T^{\pm})=\mathcal{V}^{\pm}$,
which is for instance the case when separating finite point clouds:
corresponding operators are projections $P_{X^{\pm}}$. We define:
\[
T=\min(T^{+},T^{-}),
\]
which is also a Shapley. We denote by $\perp$ the vector of $\mathbb{R}_{\max}^{d}$
identically equal to $-\infty$. The \emph{spectral radius} of $T$
is, by definition:
\[
\rho(T)=\sup\left\{\lambda\in\mathbb{R}_{\max},\quad\exists u\in\mathbb{R}_{\max}^{d}\backslash\{\perp\},\quad T(u)=\lambda+u\right\}.
\]

Observe that $\mathcal{S}(T)$ is equal to $\mathcal{S}(T^{+})\cap\mathcal{S}(T^{-})$,
i.e. the intersection of the convex hulls we are seeking to separate.
From Allamigeon, Gaubert et al. \cite{Allamigeon2018}, we know that $\mathcal{V}^{+}$
and $\mathcal{V^{-}}$ being disjoint is equivalent to $\rho(T)\le0$.

Given $a$ the corresponding eigenvector, we define the sectors:
\[
I^{\pm}:=\left\{i\in[d],\quad T^{\pm}(a)_{i}>\lambda+a_{i}\right\},
\]
and the corresponding configuration $\sigma=\{I^{+},I^{-}\}$.
\begin{prop}
Hyperplane $\mathcal{H}_{a}^{\sigma}$, given the sectors defined
above, separates $\mathcal{V}^{+}$ and $\mathcal{V}^{-}$ with a
margin of $-\rho(T)$. Moreover, this margin is optimal when $T^{\pm}$
are of the form
\[
\sup_{v\in\mathcal{V}^{\pm}}\left(v_{i}+\min(-v+x)\right),
\]
which is in particular the case when separating finite point clouds.
\end{prop}

\begin{proof}
As $T^{\pm}$ is non-expansive, let's first remark that for $x^{\pm}\in V^{\pm}$:
\[
x_{i}^{\pm}\le T^{\pm}(x^{\pm})_{i}=\left(T^{\pm}(x^{\pm})-T^{\pm}(a)\right)_{i}+T^{\pm}(a)_{i},
\]
hence 
\begin{equation}
x_{i}^{\pm}\le\max(x^{\pm}-a)+T^{\pm}(a)_{i}.\label{eq41}
\end{equation}
For instance, let $i\in[d]\setminus I^{+}$. Then $T^{+}(a)_{i}=\lambda+a_{i}$,
so for $x^{+}\in V^{+}$, using equation \ref{eq41}:
\[
x_{i}^{+}-a_{i}\le\max(x^{+}-a)+\lambda.
\]
In particular, $x_{i}^{+}-a_{i}<\max(x^{+}-a)$ and any element of
$V^{+}$ can't belong to any of sectors in $[d]\setminus I^{+}$ with
respect to $H_{a}$, from which the sectors $I^{\pm}$ are well-defined.
Finally, 
\[
d(H_{a}^{I^{+}},x^{+})=\max(x^{+}-a)-\max(x^{+}-a)_{[d]\setminus I^{+}}\ge-\lambda,
\]
and the margin comes from the fact that this applies to any element
of $V^{+}$.

Let's finally prove that the margin is maximal in the case where $T^{\pm}$
are of the form $T^{\pm}(x)=P_{V^{\pm}}(x)=\sup_{v\in V^{\pm}}\left(v_{i}+\min(-v+x)\right).$
Let $i\in[d]\setminus I^{+}$. Then, for $\varepsilon>0$, we can
find $v\in V^{+}$ such that 
\[
T^{+}(a)_{i}-\varepsilon\le v_{i}-\max(v-a)\le T^{+}(a)_{i},
\]
giving us 
\[
\lambda-\varepsilon\le v_{i}-a_{i}-\max(v-a)\le\lambda.
\]
Maximizing over all $i\in[d]\setminus I^{+}$ yields that $v$ is
at most at distance $-\lambda+\varepsilon$ of $H_{a}^{I}$, hence
the optimality.
\end{proof}
%
To build a practical algorithm, we still need to compute the spectral
radius of $T$. It amounts to solving mean-payoff games, and we can
for instance apply the following Krasnoselskii-Mann iteration scheme,
given any initial point $a^{0}$:
\[
\begin{cases}
z^{k+1} & =\frac{1}{2}\left(a^{k}+T(a^{k})\right)\\
a^{k+1} & =z^{k+1}-\max_{i\in[d]}z_{i}^{k+1}
\end{cases}
\]
The eigenpair we search is $(a^{\infty},2\cdot\max_{i\in[d]}z_{i}^{\infty})$.
Example results of this algorithm on a toy dataset are shown in Figure
\ref{MaxMarginHyperplane}.

\section{Geometric approach for soft-margin SVM}

We seek to separate convex hulls $\mathcal{V}^{+}$ and $\mathcal{V}^{-}$.
Assuming that the data overlap, we want to build a measure of their
non-separability. Using the operator $T$ we previously defined, we
know from Allamigeon, Gaubert et al. \cite{Allamigeon2018} that $\rho(T)$ is the
(positive) inner radius of $\mathcal{S}(T)$, i.e. the supremum of
the radii of the Hilbert balls contained in the intersection between
$\mathcal{V}^{+}$ and $\mathcal{V}^{-}$. The next Proposition shows
us that this measure can be interpreted geometrically, in the case
where $\mathcal{V}^{+}$ and $\mathcal{V}^{-}$ are generated by point
clouds $X^{+}$ and $X^{-}$:
\begin{prop}
By moving some points from $X^{+}$ and $X^{-}$ by a distance of
at most $\rho(T)$, we can nullify the interior of the intersection
of the tropical spans.

More specifically, we note $a\in\mathbb{R}_{\max}^{d}$ the eigenvector
corresponding to $\rho(T)$. We project all points of $X^{+}$ and
$X^{-}$ whose distance to $\mathcal{H}_{a}$ is less than $\rho(T)$,
onto $\mathcal{H}_{a}$. Then the intersection of new convex hulls
is of empty interior.
\end{prop}

\begin{proof}
Let's denote $X$ the cloud consisting of all points (regardless of
sign), and for $x_{j}\in X$, we note $y_{j}$ its sign, $s_{j}$
its sector and $d_{j}$ the second argmax of $(x_{j}-a)$. For each
point $x_{j}=X_{\cdot j}\in X$ at distance less than $\lambda$ of
$H_{a}$, the transformation consists in setting 
\[
W_{kj}:=\begin{cases}
X_{kj} & \text{if \ensuremath{k\ne s_{j}}}\\
X_{s_{j}j}-d(x_{j},H_{a}) & \text{at \ensuremath{s_{j}}}
\end{cases}
\]
so that $w_{j}$ is projected on the hyperplane. As $T^{\pm}$ is
non-expansive and diagonal-free, let's remark that for $x^{\pm}\in V^{\pm}$:
\[
x_{i}^{\pm}\le T^{\pm}(x^{\pm})_{i}=\left(T^{\pm}(x^{\pm})-T^{\pm}(a)\right)_{i}+T^{\pm}(a)_{i},
\]
hence
\begin{equation}
x_{i}^{\pm}\le\max(x^{\pm}-a)_{\ne i}+T^{\pm}(a)_{i}.\label{eq31}
\end{equation}
Let $i\in[d]$. If $x_{j}\in X$ is not in the $i$-th sector, then
for $k$ different from the sector of $x_{j}$, by definition: 
\[
(w_{j}-a)_{k}\le(x_{j}-a)_{d_{j}}\le(w_{j}-a)_{s_{j}},
\]
hence 
\[
W_{ij}-\max_{k\ne i}\left(W_{kj}-a_{k}\right)\le a_{i}.
\]
Otherwise, $x_{j}$ is in the $i$-th sector and: 
\[
\max_{k\ne i}\left(W_{kj}-a_{k}\right)=X_{d_{j}j}-a_{d_{j}},
\]
thus 
\[
W_{ij}-\max_{\ne i}\left(w_{j}-a\right)=\left(w_{j}-a\right)_{s_{j}}-\left(x_{j}-a\right)_{d_{j}}+a_{s_{j}}\ge a_{s_{j}}=a_{i},
\]
with equality iff $d(x_{j},H_{a})\leq\lambda$.

Suppose by symmetry that $T(a)_{i}=T^{+}(a)_{i}=\lambda+a_{i}.$ We
also have $T^{-}(a)_{i}\ge\lambda+a_{i}$. Then, using the proof of
Theorem 22 in \cite{Akian2021TropicalLR}, we know that there exists
$j^{+},j^{-}\in[p]$ such that $x_{j^{+}}\in X^{+}$ and $x_{j^{-}}\in X^{-}$
are in sector $i$, with $x_{j^{+}}$ being at distance $\lambda$
from $H_{a}$ and $x_{j^{-}}$ at distance greater than $\lambda$.
Therefore, $W_{ij^{+}}-\max_{\ne i}\left(w_{j^{+}}-a\right)=a_{i}$
and $W_{ij^{-}}-\max_{\ne i}\left(w_{j^{-}}-a\right)\geq a_{i}$.
Moreover, for any $j$ such that $x_{j}\in X^{+}$ is in sector $i$,
eq. \ref{eq31} gives $d(x_{j},H_{a})\leq\lambda$.

Let $Q^{\pm}$ be the \emph{diagonal-free} projections over transformed
point clouds, and

$Q=Q^{+}\wedge Q^{-}.$ We've just shown that $Q(a)_{i}=Q^{+}(a)_{i}=a_{i}$,
and finally $Q(a)=a$.
\end{proof}
%
In pseudo-polynomial time, we are thus able to compute a distance
of our point clouds to separability. It gives a bound on the distance
under which some points have to be moved to separate hulls.

As $a$ is the center of the inner ball of $\mathcal{V}^{+}\cap\mathcal{V}^{-}$,
we also have a purely geometric heuristic for determining a separating
hyperplane in the non-separable case: we simply take the hyperplane
$\mathcal{H}_{a}$ and then assign the sectors based on majoritary
population, for instance.


\section{Topics Being Explored}

\subsection{Hard-Margin Multi-Classification}

In this section, we consider convex hulls of point clouds of $n$
classes, noted $V^{k}$ for $k\in[n]$, and described by Shapley operators
$T^{k}$. We give a sufficient condition for these classes to be tropically
separable in their whole, meaning that there exists a tropical signed
hyperplane such that each $V^{k}$ belong to sectors of $I^{k}\subset[d]$
with $I^{k}\cap I^{l}=\emptyset$ for $k\ne l\in[n].$ We then adapt
the previous results to this case.

We now consider the Shapley operator :

\[
T:=\bigvee_{1\le k<l\le n}T^{k}\wedge T^{l}
\]

We remark that in the case $n=2$, $T$ is the same as previously
defined.

Let $(a,\lambda)$ the eigenpair of $T$ approximated by the Kranoselskii-Mann
algorithm, verifying $T(a)=\lambda+a$. Let's define the sectors:
\[
I^{k}:=\{i\in[d],\quad T^{k}(a)_{i}>\lambda+a_{i}\}.
\]

Given the definition of $T$, it is clear that $I^{k}\cap I^{l}=\emptyset$
for $k\ne l\in[n],$ and we have the following result :
\begin{prop}
If $\lambda<0$, the tropical parametrized hyperplane $H_{a}^{\sigma}$,
given the configuration $\sigma=\{I^{k}\}_{k\in[n]}$, separates $V^{k}$
and $V^{l}$ for all $k\ne l\in[n]$ with a margin of $-\lambda$.
Moreover, this margin is optimal in the case where all $T^{k}$ are
of the form
\[
T^{k}(x)=P_{V^{k}}(x)=\sup_{v\in V^{k}}\left(v_{i}+\min(-v+x)\right),
\]
 which is in particular the case when separating finite point clouds. 
\end{prop}

\begin{proof}
Let $k\in[n]$ and $i\in[d]\setminus I_{k}$. Using the same reasoning
as in the proof of Proposition $19$, we obtain that for all $x^{k}\in V^{k}$,
\[
d(H_{a}^{\sigma},x^{k})=\max(x^{k}-a)-\max(x^{k}-a)_{[d]\setminus I_{k}}\ge-\lambda,
\]
hence the margin. Let's now prove the optimality in the case where
all $T^{k}$ are of the form $T^{k}(x)=P_{V^{k}}(x)=\sup_{v\in V^{k}}\left(v_{i}+\min(-v+x)\right).$
For all $i\in[d]$, there are two distinct classes $k\ne l\in[n]$
such that $T^{k}(a)_{i}\wedge T^{l}(a)_{i}=\lambda+a_{i}$. As $I^{k}\cap I^{l}=\emptyset$,
we can suppose by symmetry that $i\in[d]\setminus I_{k}.$ Then using
the same argument as in the proof of optimality in Proposition $19$,
we know that for all $\varepsilon>0$ there is a point $v^{k}\in V^{k}$
such that $\max(v^{k}-a)-(v_{i}^{k}-a_{i})\le-\lambda+\varepsilon$,
which means that 
\[
d(H_{a}^{\sigma},x^{k})=\max(x^{k}-a)-\max(x^{k}-a)_{[d]\setminus I_{k}}\le-\lambda+\varepsilon.
\]

As this holds for every sector $i\in[d]$, we have proven the optimality. 
\end{proof}

\subsection{Adding Features: Tropically Polynomial Decision Boundaries}

In the classical setting, the kernel trick consists in mapping the
training points in a higher-dimensional space in which we expect the
data to become easily linearly separable. In this paragraph, we adapt
this idea to integer combinations of features in the tropical setting.

If $\mathcal{A}\subset\mathbb{Z}^{d}$ is a set of vectors, we define
the \emph{veronese embedding} of $x$ as 
\[
\text{ver}(x):=\left(\langle x,\alpha\rangle\right)_{\alpha\in\mathcal{A}}\in\mathbb{R}^{\mathcal{A}},
\]
allowing us to map our point space into a larger space, made up of
various integer combinations of features. 

To take a fairly exhaustive example, let's consider the integer combinations defined by the integer points of the dilated simplex. Noting $s\in\mathbb{N}$ a scale parameter, and $\Delta_{d}$ the $d$-dimensional simplex, we define 
\[
\mathcal{A}_{s}:=(s\Delta_{d})\cap\mathbb{Z}^{d}.
\]

\begin{prop}
Applying the previous method to point clouds $\text{ver}(C_{i})$
for each class $i$ yields a classifier whose decision boundaries,
when seen in the initial vector space, are tropical polynomials. (cite
?)
\end{prop}

\begin{prop}
\emph{(Zhang et al. \cite{zhang2018tropical})} Feedforward neural networks with rectified linear units have decision boundaries that are, modulo trivialities, nothing more than tropical rational maps, i.e differences of tropical polynomials. 
\end{prop}

Thus, the Veronese embedding method brings us closer to the behaviour of dense neural networks.

\begin{example}
We test the relevance of our Veronese embedding method on two classic data sets.

The \emph{Iris flower} dataset includes measurements of sepal length, sepal width, petal length, and petal width, across three species of Iris (Iris setosa, Iris virginica, and Iris versicolor). The \emph{Wine quality and type dataset} comprises physicochemical tests (like alcohol content, acidity,
sugar level, etc.) and sensory information (quality score) for various samples of red and white wines.

After applying our algorithm, we assign the sectors to the majority population.

\begin{figure}[!h]
\begin{tabular}{|c|c|c|c|}
\hline 
Dataset  & Groups  & $d$  & $p$\tabularnewline
\hline 
\hline 
Iris  & Setosa, Virginica, Versicolor  & 4  & 150\tabularnewline
\hline 
Wine  & Red (bad), Red (good), White (bad), White (good)  & 10  & 6500\tabularnewline
\hline 
\end{tabular}

\caption{Datasets description}
\end{figure}

\begin{figure}[!h]
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline 
\multirow{2}{*}{Dataset} & \multicolumn{2}{c|}{$\mathcal{A}_{1}$} & \multicolumn{2}{c|}{$\mathcal{A}_{2}$} & \multicolumn{2}{c|}{$\mathcal{A}_{3}$} & \multicolumn{2}{c|}{$\mathcal{A}_{4}$}\tabularnewline
\cline{2-9} \cline{3-9} \cline{4-9} \cline{5-9} \cline{6-9} \cline{7-9} \cline{8-9} \cline{9-9} 
 & \emph{1v1}  & \emph{1vR}  & \emph{1v1}  & \emph{1vR}  & \emph{1v1}  & \emph{1vR}  & \emph{1v1}  & \emph{1vR} \tabularnewline
\hline 
Iris  & 70\%  & 87\%  & 93\%  & 93\%  & 90\%  & 87\%  & 87\%  & 87\%\tabularnewline
\hline 
Wine  & 67\%  & 56\%  & 72\%  & 74\%  & 78\%  & 85\%  & 87\%  & 92\%\tabularnewline
\hline 
\end{tabular}

\caption{Accuracies for one-vs-one and one-vs-rest classifiers}
\end{figure}

In the table, the dots indicate overfitting, or that the dimension
has become too large for the calculations to succeed. The addition
of complexity by this kernel method seems particularly effective for
the wine dataset.

\end{example}

\begin{rem}
The sets $A_{s}$ grow exponentially with the dimension and polynomially
with $s$, and encode the complexity of the mimicked neural network.
$s$ will therefore be a relevant hyperparameter of overfitting or
underfitting.

We note that irrelevant features are driven out of the decision process
by a correspondingly large coordinate in the apex, preventing this
feature from ever being maximal. This could guide us towards heuristics
to remove superfluous features, combat overfitting and reduce training
time.
\end{rem}

\begin{figure}
    \centering
    \resizebox{0.7\textwidth}{!}{\input{fig/A_1.pgf}}
    \caption{$\mathcal{A}_1$ doesn't separate blue class well}
    \label{fig:veronese_A1}
\end{figure}

\begin{figure}
    \centering
    \resizebox{0.7\textwidth}{!}{\input{fig/A_2.pgf}}
    \caption{$\mathcal{A}_2$ is better}
    \label{fig:veronese_A2}
\end{figure}

\begin{figure}
    \centering
    \resizebox{0.7\textwidth}{!}{\input{fig/A_3.pgf}}
    \caption{$\mathcal{A}_3$ overfits data}
    \label{fig:veronese_A3}
\end{figure}

\begin{example}
In Figures \ref{fig:veronese_A1}, \ref{fig:veronese_A2} and \ref{fig:veronese_A3}, we visualize the decision boundaries of the Iris classifier depending on scale parameter $s$. We remove the last feature of this dataset to have visualizable, 3-dimensional data.

For the last two figures, the tropical polynomial represented was rendered with Polymake, and the overlay with the data points was done by hand in an approximate - but very representative - manner.
\end{example}

\subsection{Linear Hyperplanes Look Tropical on Log Paper}

Let $X_{ij}$ be our point clouds, and for $\beta>0$, let's define
$x^{\beta}=(x_{ij}^{\beta}:=e^{\beta X_{ij}})_{ij}$. We will show
that separating different classes from $x^{\beta}$ using a linear
SVM, when $\beta$ tends toward infinity, yields a separating hyperplane
that converges towards a tropical hyperplane in the initial space.

Our method, by directly outputting an optimal-margin separating tropical
hyperplane in pseudo-polynomial time, is expected to achieve better
results. If $\beta$ is small, we might indeed be far away from the
limiting tropical hypersurface; conversely, as $\beta$ approaches
infinity, numerical error becomes predominant.

\begin{figure}[h]
\centering \includegraphics[scale=0.1]{\string"fig/linear log-exp kernel\string".png}
\includegraphics[scale=0.1]{\string"fig/tropical svm\string".png} 
\end{figure}


\subsubsection{Tropical hyperplanes are limiting classical hyperplanes}

Under the assumption that the $x_{ij}^{\beta}$ are linearly separable
\textbf{\emph{(strong assumption because it has to hold for all values
of $\beta$)}}, we compute a support vector classifier without intercept
term, whose separation surface's equation is: 
\[
H^{\beta}:\quad w^{\beta}\cdot x=0,
\]
where all positive (resp. negative) points verify $w^{\beta}\cdot x\ge1$
(resp. $w^{\beta}\cdot x\le-1$). We write 
\[
w_{i}^{\beta}=\sigma_{i}e^{\beta W_{i}^{\beta}},
\]
where $\sigma_{i}\in\{\pm1\}$ and $W_{i}^{\beta}\in\mathbb{R}\cup\{-\infty\}$.
For now, we simplify the study by considering a fixed $W_{i}$ value
and $w_{i}^{\beta}=\sigma_{i}e^{\beta W_{i}}$. 
\begin{lem}
(Maslov's sandwich) For $\beta>0$ and $I\subset[d]$ we have: 
\[
0\leq\beta^{-1}\log\left(\sum_{i\in I}e^{\beta(W_{i}+X_{ij})}\right)-\max_{i\in I}(W_{i}+X_{ij})\le\beta^{-1}\log d.
\]
\end{lem}

\begin{proof}
Let $\beta>0$ and $I\subset[d]$. We have: 
\[
\exp\left\{ \beta\max_{i\in I}\left(W_{i}+X_{ij}\right)\right\} \le\sum_{i\in I}e^{\beta(W_{i}+X_{ij})}\le d\cdot\exp\left\{ \beta\max_{i\in I}\left(W_{i}+X_{ij}\right)\right\} ,
\]
hence the result by taking the logarithm and dividing by $\beta$. 
\end{proof}
\begin{prop}
Defining $H^{\text{trop}}$ as the hyperplane of apex $(-W_{i})_{i\in[d]}$,
signed using $I^{+}:=\{i\in[d],\quad\sigma_{i}>0\}$ and $I^{-}:=[d]\backslash I^{+}$,
we have: 
\[
d_{H}\left(\log H^{\beta},H^{\text{trop}}\right)\le\beta^{-1}\log d,
\]
where $d_{H}$ is the tropical Haussdorf distance. Hence $\log H^{\beta}\longrightarrow H^{\text{trop}}$
as $\beta\longrightarrow+\infty$. 
\end{prop}

\begin{proof}
Let $\beta>0$ and $X\in H^{\beta}$. By writing the inequalities
from previous lemma with $I^{+}$ and $I^{-}$, and as $$\beta^{-1}\log\left(\sum_{i\in I^{+}}e^{\beta(W_{i}+X_{ij})}\right)=\beta^{-1}\log\left(\sum_{i\in I^{-}}e^{\beta(W_{i}+X_{ij})}\right),$$
substracting the first to second inequality yields: 
\[
-\beta^{-1}\log d\le\max_{i\in I^{+}}(W_{i}+X_{ij})-\max_{i\in I^{-}}(W_{i}+X_{ij})\le\beta^{-1}\log d.
\]
hence $d(X,H^{\text{trop}})\le\beta^{-1}\log d$.

Reciprocally, let $Y\in H^{\text{trop}}$ and $X=Y+\delta\mathbf{1}_{I^{+}}$,
with $\delta$ to be defined later. To have $Y\in H^{\beta}$, we
have to ensure that: 
\[
w^{\beta}\cdot y=0,
\]
which amounts to 
\[
\sum_{i\in[d]}\sigma_{i}e^{\beta W_{i}}e^{\beta X_{i}}=0.
\]
By separating the positive and negative terms, and taking the logarithm,
we get 
\[
\beta\delta+\log\left(\sum_{i\in I^{+}}e^{\beta(W_{i}+X_{ij})}\right)=\log\left(\sum_{i\in I^{-}}e^{\beta(W_{i}+X_{ij})}\right),
\]
which similarily yields 
\[
\delta\le\left|\beta^{-1}\log\left(\sum_{i\in I^{+}}e^{\beta(W_{i}+X_{ij})}\right)-\beta^{-1}\log\left(\sum_{i\in I^{-}}e^{\beta(W_{i}+X_{ij})}\right)\right|\le\beta^{-1}\log d,
\]
hence $d(Y,H^{\beta})\le\beta^{-1}\log d$. 
\end{proof}
\begin{rem}
We note $L$ the order of magnitude of our data points, and $B$ a
typical number at which the computer starts having numerical errors
or overflow (typically, for C integer calculations, $B=2^{16}-1)$.
We want to have $\beta L\ll\log B$ (that is, no numerical error),
and $\beta^{-1}\log d\ll L$ (good convergence towards tropical hyperplane),
i.e $d\ll B$. By directly computing logarithms, for instance, our
method should be very suitable for $d\apprge65535$ dimensions using
C integers. 
\end{rem}


\subsubsection{Finding tropical apex}

In the classical hard-margin setting, admissible $w$ vectors verify
$w^{T}x^{+}\ge1$ (resp. $w^{T}x^{-}\le-1$) for positive (resp. negative)
vectors. Thus they belong to the polytope $P^{\beta}$ where 
\[
P^{\beta}:=\left\{ w\in\mathbb{R}^{d},\quad w^{T}x_{j_{+}}^{\beta}\ge1\text{ and }w^{T}x_{j_{-}}^{\beta}\le-1,\forall j_{+},j_{-}\in J^{+},J^{-}\right\} .
\]

We hope that $L_{\sigma}(P^{\beta}):=\left\{ (\beta^{-1}\log(\sigma_{i}w_{i}))_{i\in[d]}\right\} $
converges towards the corresponding limiting tropical polytope $P_{\sigma}^{\text{trop}}:=P_{\sigma,+}^{\text{trop}}\cap P_{\sigma,-}^{\text{trop}}$,
where 
\[
P_{+,\sigma}^{\text{trop}}:=\left\{ W\in(\mathbb{R}\cup\{-\infty\})^{d},\quad\max_{i,\sigma_{i}=1}(W_{i}+X_{ij}^{\sigma})\ge\max_{i,\sigma_{i}=-1}(W_{i}+X_{ij}^{\sigma})\vee0,\forall j\in J^{+}\right\} .
\]
and 
\[
P_{-,\sigma}^{\text{trop}}:=\left\{ W\in(\mathbb{R}\cup\{-\infty\})^{d},\quad\max_{i,\sigma_{i}=-1}(W_{i}+X_{ij}^{\sigma})\le\max_{i,\sigma_{i}=1}(W_{i}+X_{ij}^{\sigma})\vee0,\forall j\in J^{-}\right\} .
\]

\begin{thm}
(Cite the corresponding article) When points $x_{ij}^{\beta}$ are
in a general position, \textbf{(clarify what this means as $\beta$}
\textbf{approaches infinity)} 
\[
\lim_{\beta\rightarrow+\infty}L_{\sigma}(P^{\beta})=P_{\sigma}^{\text{trop}},
\]
with respect to the Haussdorf distance. 
\end{thm}

We proved the convergence of the logarithm of linear hypersurfaces
towards a tropical limiting hypersurface, when the apex is fixed.
However, in practice, there is an underlying double limit here: for
each $\beta$ value, we compute an apex in the exponentialized space
and we hope it will converge towards a fixed apex in the initial space:
let's consider $w_{i}^{\beta}=\sigma_{i}e^{\beta W_{i}^{\beta}}$
again. 
\begin{thm}
(Cite the corresponding article) 
\[
W^{\infty}:=\lim_{\beta\rightarrow+\infty}\beta^{-1}\log w^{\beta}\in\underset{W\in P^{\text{trop}}}{\arg\min}\left(\max w\right).
\]
\end{thm}


\subsubsection{Computing limiting margin (experimental)}

In the classical setting, the margin is $\lVert w\rVert^{-1}$, described
by the vector $w/\lVert w\rVert^{2}$. Conjecturing that this vector,
when mapped back to the initial space, gives a meaningful approximation
of the margin (prove it), we can compute the norm of this limiting
vector. For $\beta>0$: 
\begin{align*}
\beta^{-1}\log\left(\frac{w^{\beta}}{\lVert w^{\beta}\rVert^{2}}\right) & =\beta^{-1}\log w^{\beta}-2\log\lVert e^{\beta W^{\beta}}\rVert^{\beta^{-1}}.
\end{align*}
Where the term $\log\lVert e^{\beta W^{\beta}}\rVert^{\beta^{-1}}\rightarrow\max\left|w^{\beta}\right|$,
and $\beta^{-1}\log w^{\beta}\rightarrow W^{\infty}$. Hence: 
\[
\left\lVert \beta^{-1}\log\left(\frac{w^{\beta}}{\lVert w^{\beta}\rVert^{2}}\right)\right\rVert \longrightarrow\max W^{\infty}-\min W^{\infty}
\]
would give a good approximation of the limiting margin.

\newpage{} \bibliographystyle{plain}
\bibliography{ea}

\end{document}
