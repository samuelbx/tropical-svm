\documentclass[oneside,english,a4paper]{amsart}
\usepackage{bbold}
\usepackage[T1]{fontenc}
\usepackage{inputenc}
\setlength{\parskip}{\smallskipamount}
\setlength{\parindent}{0pt}
\usepackage{amstext}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{pgfplots}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{wasysym}
\usepackage{fullpage}

\makeatletter
\numberwithin{equation}{section}
\numberwithin{figure}{section}
\theoremstyle{plain}
\newtheorem{thm}{\protect\theoremname}
\theoremstyle{definition}
\newtheorem{defn}[thm]{\protect\definitionname}
\theoremstyle{plain}
\newtheorem{prop}[thm]{\protect\propositionname}
\theoremstyle{remark}
\newtheorem{rem}[thm]{\protect\remarkname}
\theoremstyle{plain}
\newtheorem{lem}[thm]{\protect\lemmaname}
\theoremstyle{definition}
\newtheorem{example}[thm]{\protect\examplename}
\newtheorem{cor}[thm]{\protect\corollaryname}
\theoremstyle{definition}

\makeatother

\usepackage{babel}
\providecommand{\definitionname}{Definition}
\providecommand{\lemmaname}{Lemma}
\providecommand{\propositionname}{Proposition}
\providecommand{\remarkname}{Remark}
\providecommand{\theoremname}{Theorem}
\providecommand{\examplename}{Example}
\providecommand{\corollaryname}{Corollary}

\begin{document}
\title{Tropical Piecewise Linear Classifiers and mean payoff games}
\author{Xavier Allamigeon, Stéphane Gaubert, Samuel Boïté, Théo Molfessis}
\maketitle

\begin{abstract}
    We develop new algorithms around tropical support vector machines (SVMs) for binary and multi-classification. We explore the use of tropical hyperplanes to partition tropical space, laying the groundwork for our classification approach. We address the challenge of separating overlapping data in tropical space, proposing a method to measure and handle data overlap using non-expansive Shapley operators and a Krasnoselskii-Mann iteration scheme. For separable data, we extend our method to determine hard-margin tropical hyperplanes, ensuring maximal separation. This is further applied to multi-classification scenarios, providing conditions for tropical separability and demonstrating margin optimality. Additionally, the tropical kernel trick is explored as a means to embed data into a higher-dimensional tropical space, transforming decision boundaries into tropical polynomials. This approach is empirically tested on several classic datasets. Finally, we show that tropical hyperplanes emerge as limiting cases of classical hyperplanes on logarithmic paper, making our approach more stable numerically.
\end{abstract}

\section{Introduction}

\subsection*{The tropical linear classification problem}

% TODO: add an introduction to explain the relevance of the topic

The tropical \emph{max-plus} semifield $\mathbb{R}_{\max}$ is the
set of real numbers, completed by $-\infty$ and equipped with the
addition $a\oplus b=\max(a,b)$ and the multiplication $a\odot b=a+b$.
A \emph{tropical hyperplane} in the $d$-dimensional tropical vector
space $\mathbb{R}_{\max}^{d}$ is a set of vectors of the form
\[
\mathcal{H}_{a}=\left\{x\in\mathbb{R}_{\max}^{d},\quad\max_{1\le i\le d}(a_{i}+x_{i})\,\text{is achieved at least twice}\right\}.
\]
Such a hyperplane is parametrized by the vector $a=(a_{1},\ldots,a_{d})\in\mathbb{R}_{\max}^{d}$,
which is required to be non-identically $-\infty$. It hence partitions
the space between $d$ different \emph{tropical sectors}, laying the
ground for multi-class classification. By definition, it is invariant by translation along the constant vector $(1,\ldots, 1)$.

In this paper, we address the following tropical analogue of support
vector machines. Given $n\in\{1,\ldots, d\}$ classes of $d$-dimensional data
points $X^{1},\ldots,X^{p}$, we look for a maximum-margin separating
tropical hyperplane to build a classifier. The notion of margin depends
on the metric: the canonical choice in tropical geometry is the (additive
version of) Hilbert's projective metric. Its restriction to $\mathbb{R}^{d}$
is induced by the so-called \emph{Hilbert's seminorm} or \emph{Hopf
oscillation}
\[
\lVert x\rVert_{H}:=\max_{1\le i\le d}x_{i}-\min_{1\le i\le d}x_{i}.
\]
It is a projective metric, in the sense that the distance between
two points is zero if and only if these two points differ by an additive
constant. When dealing with hard-margin support vector machines (SVMs),
we also have to ensure that every point lands in the right sector.
We define the \emph{tropical parametrized hyperplane} of configuration
$\sigma=\left\{I^{1},\ldots,I^{n}\right\}$, where $I^{k}$ and $I^{\ell}$
are disjoint for $k\ne\ell$, as:
\[
\mathcal{H}_{a}^{\sigma}=\left\{x\in\mathbb{R}_{\max}^{d},\quad\exists k\ne\ell,\quad (x-a)\,\text{reaches its max coordinate in \ensuremath{I^{k}} and \ensuremath{I^{\ell}}}\right\}.
\]

Hence, $\mathcal{H}_{a}^{\sigma}$ is said to \emph{separate} point
clouds $(X^{k})_{k\in[p]}$ with a margin $\nu$ when for every point
$x^{k}$ of class $k$:
\begin{enumerate}
\item $x^{k}$ is on the right sector of the hyperplane, i.e its maximum
coordinate is achieved in $I^{k}$
\item $x^{k}$ is at distance at least $\nu$ from $\mathcal{H}_{a}^{\sigma}$:
\begin{equation*}
d_H(\mathcal{H}_{a}^{\sigma},x^{k})=\max(x^{k}-a)-\max_{\sqcup_{\ell\ne k}I^{\ell}}(x^{k}-a)\ge\nu.
\end{equation*}
\end{enumerate}
In Figure \ref{fig:MaxMargin}, we separate two classes
of $3$-dimensional points from a toy tropical dataset using a tropical
hyperplane. Its margin is maximal and is represented by the Hilbert
ball in gray. The figure is represented in the projective space $\mathbb{P}\left(\mathbb{R}_{\text{max}}^{3}\right)$,
i.e. the quotient of the set of non-identically $-\infty$ vectors
of $\mathbb{R}_{\text{max}}^{3}$ by the equivalence relation which
identifies tropically proportional values.

Since our separator is translation-invariant by the constant vector, in order to separate any $d$-dimensional dataset, we plunge it into the $x_1+\cdots+x_{d+1}=0$ subspace of $\mathbb{R}_{\max}^{d+1}$. This amounts to artificially adding a new coordinate to each point, equal to the opposite of the sum of the initial coordinates. If we're dealing with an intrinsically tropical dataset, for which this translation invariance makes sense, it is better to stay in the original space, that is to classify directly on the projective space.

\begin{figure}
\input{figures/bintoy-separated_1.pgf}
\label{fig:MaxMargin}
\caption{Maximum-margin tropical parametrized hyperplane, binary setting}
\end{figure}


\subsection*{Results}

TBD

\section{Tropical Support Vector Machines}\label{Sec2}

\subsection{Preliminaries on mean payoff games}

In this section, we have a finite set of points $X=(x_{1},\ldots,x_{p})\in\mathbb{R}_{\text{max}}^{d\times p}$.
We define the \emph{tropical span} of $X$ as the set of tropical
linear combinations of these points:
\[
\text{Span}(X):=\left\{\max_{1\le i\le p}(x_{i}+\lambda_{i}),\quad\lambda\in\mathbb{R}^{p}\right\}.
\]
As this is also the smallest tropically convex set containing these
points, separating several point clouds amounts to separating their
tropical spans. Given a tropically convex, compact and nonempty subset $\mathcal{V}$
of $\mathbb{R}_{\max}^{d}$, we define the projection of any vector
$x$ in $\mathbb{R}_{\max}^{d}$ by:
\[
P_{\mathcal{V}}(x):=\max\{y\in \mathcal{V},\quad y\le x\}.
\]
When $\mathcal{V}$ is the tropical span of $X$, we will also note
this projection as $P_{X}$. We know from Maclagan et al. \cite{Maclagan2015} that tropical projections
can be expressed as a mean payoff game: for $i\in[p]$ and $x\in\mathbb{R}_{\max}^{d}$,
\begin{equation}
P_{X}(x)_{i}=\max_{j\in[p]}\left\{X_{ij}+\min_{1\le k \le d}(-X_{kj}+x_{k})\right\}.\label{eq:projector}
\end{equation}

Here, we recognize the payoff of a zero-sum deterministic game with perfect
information. There are two players, ``Max'' and ``Min'' (the maximizer
and the minimizer), who alternate their actions. Starting on node
$i$, Player Max chooses to move to node $j$, and receives $X_{ij}$
from Player Min. Similarily, Player Min in turn chooses a node $k$
and has to pay $-X_{kj}$ to Player Max.

We now recall some tools from Perron-Frobenius theory, in relation
with mean payoff games. We refer the reader to \cite{AKIAN2012} for more
information. We call \emph{Shapley operator} a map $T:\mathbb{R}_{\max}^{d}\rightarrow\mathbb{R}_{\max}^{d}$
that is order preserving, continuous, and such that $T(\alpha+x)=\alpha+T(x)$
for all $\alpha\in\mathbb{R}_{\max}$ and $x\in\mathbb{R}_{\max}^{d}$. $T$ is said to be \emph{diagonal free} when $T_{i}(x)$
is independent of $x_{i}$ for all $i\in\{1,\ldots, n\}$, i.e. when for all $x,y\in\mathbb{R}_{\max}$ such that $x_{j}=y_{j}$
for all $j\neq i$, we have $T_{i}(x)=T_{i}(y)$. 

Observe that the tropical projection defined by \ref{eq:projector}
is a Shapley operator. Given a Shapley operator $T$, we also define
\[
\mathcal{S}(T):=\left\{x\in\mathbb{R}_{\max}^{d},\quad x\le T(x)\right\}.
\]
More specifically, any tropically convex set $\mathcal{V}$ is equal
to $\mathcal{S}(P_{\mathcal{V}})$. Indeed, as for any $x\in\mathbb{R}_{\text{max}}^{d}$,
$P_{\mathcal{V}}(x)\le x$, having $x\in\mathcal{S}(P_{\mathcal{V}})$
implies $x=P_{\mathcal{V}}(x)$, meaning $x$ is in
$\mathcal{V}$.


Operator $P$ can be slightly tweaked to make it \emph{diagonal-free} -- but the resulting operator isn't a projector anymore.
In the modified game, the opponent is prevented from replying eye-for-an-eye:
\[
\left[P^{\text{DF}}_X(x)\right]_{i}:=\max_{j\in[p]}\left\{ X_{ij}+\min_{k\ne i}(-X_{kj}+x_{k})\right\} .
\]

Noticeably, $\mathcal{S}(P_X^\text{DF}) = \mathcal{S}(P_X)$.

\subsection{Hard-margin binary SVM}

In this section, we introduce a practical criterion for the existence of a separating tropical hyperplane, and if it exists, we construct one with optimal margin. We also place ourselves in the binary setting, where we seek to two separate convex hulls $\mathcal{V}^{+}$
and $\mathcal{V}^{-}$. We assume we have two Shapley operators $T^{+}$
and $T^{-}$ such that $\mathcal{S}(T^{\pm})=\mathcal{V}^{\pm}$,
which is for instance the case when separating finite point clouds:
corresponding operators are projections $P_{\mathcal{V}^{\pm}}$. We define:
\[
T=\inf(T^{+},T^{-}),
\]
which is also a Shapley. We denote by $\perp$ the vector of $\mathbb{R}_{\max}^{d}$
identically equal to $-\infty$. The \emph{spectral radius} of $T$
is, by definition:
\[
\rho(T)=\sup\left\{\lambda\in\mathbb{R}_{\max},\quad\exists u\in\mathbb{R}_{\max}^{d}\backslash\{\perp\},\quad T(u)=\lambda+u\right\}.
\]

Observe that $\mathcal{S}(T)$ is equal to $\mathcal{S}(T^{+})\cap\mathcal{S}(T^{-})$,
i.e. the intersection of the convex hulls we are seeking to separate.
From Allamigeon, Gaubert et al. \cite{Allamigeon2018}, we know that $\mathcal{V}^{+}$
and $\mathcal{V^{-}}$ being disjoint is equivalent to $\rho(T)\le0$.

Given $a$ the eigenvector corresponding to $\rho(T)$, we define the sectors:
\[
I^{\pm}:=\left\{i\in\{1,\ldots, d\},\quad T^{\pm}(a)_{i}>\rho(T)+a_{i}\right\},
\]

TODO : RÉGLER QUESTION DE INF SUR LES SECTEURS ET ENVOI DE L'APEX A +INF

and the corresponding configuration $\sigma=\{I^{+},I^{-}\}$.
\begin{prop}
Hyperplane $\mathcal{H}_{a}^{\sigma}$, given the sectors defined
above, separates $\mathcal{V}^{+}$ and $\mathcal{V}^{-}$ with a
margin of $-\rho(T)$. Moreover, this margin is optimal when $T^{\pm}$
are of the form
\[
\sup_{v\in\mathcal{V}^{\pm}}\left(v_{i}+\min(-v+x)\right),
\]
which is in particular the case when separating finite point clouds.
\end{prop}

\begin{proof}
As $T^{\pm}$ is non-expansive, for $x^{\pm}\in V^{\pm}$:
\[
x_{i}^{\pm}\le T^{\pm}(x^{\pm})_{i}=\left(T^{\pm}(x^{\pm})-T^{\pm}(a)\right)_{i}+T^{\pm}(a)_{i},
\]
hence 
\begin{equation}
x_{i}^{\pm}\le\max(x^{\pm}-a)+T^{\pm}(a)_{i}.\label{eq41}
\end{equation}
For instance, let $i\in I^{-}$. Then $T^{+}(a)_{i}=\rho(T)+a_{i}$,
so for $x^{+}\in \mathcal{V}^{+}$, using equation \ref{eq41}:
\[
x_{i}^{+}-a_{i}\le\max(x^{+}-a)+\rho(T).
\]
In particular, $x_{i}^{+}-a_{i}<\max(x^{+}-a)$ and any element of
$\mathcal{V}^{+}$ cannot belong to any of sectors in $I^{-}$ with
respect to $\mathcal{H}_{a}$, from which the sectors $I^{\pm}$ are well-defined.
Finally, 
\[
d_H(\mathcal{H}_{a}^{I^{+}},x^{+})=\max(x^{+}-a)-\max(x^{+}-a)_{I^{-}}\ge-\rho(T),
\]
hence the margin.

Let's finally prove that the margin is optimal in the case where $T^{\pm}$
are of the aforementioned form.
Let $i\in I^{-}$. Then, for $\varepsilon>0$, we can
find $v\in \mathcal{V}^{+}$ such that 
\[
T^{+}(a)_{i}-\varepsilon\le v_{i}-\max(v-a)\le T^{+}(a)_{i},
\]
giving us 
\[
\rho(T)-\varepsilon\le v_{i}-a_{i}-\max(v-a)\le\rho(T).
\]
Maximizing over all $i\in I^{-}$ gives that $v$ is
at most at distance $-\rho(T)+\varepsilon$ of $\mathcal{H}_{a}^{\sigma}$, hence
the optimality.
\end{proof}
%
To build a practical algorithm, we still need to compute the spectral
radius of $T$. It amounts to solving mean-payoff games, and we can
for instance apply the following Krasnoselskii-Mann iteration scheme,
given any initial point $a^{0}$:
\[
\begin{cases}
z^{k+1} & =\frac{1}{2}\left(a^{k}+T(a^{k})\right)\\
a^{k+1} & =z^{k+1}-\max_{1\le i\le d}z_{i}^{k+1}
\end{cases}
\]
The eigenpair we search is $(a^{\infty},2\cdot\max_{1\le i\le d}z_{i}^{\infty})$.
Example results of this algorithm on a toy dataset are shown in Figure
\ref{MaxMarginHyperplane}.

TODO : TALK ABOUT EFFICIENCY

\subsection{Geometric approach for soft-margin SVM}

We seek to separate convex hulls $\mathcal{V}^{+}$ and $\mathcal{V}^{-}$.
Assuming that the data overlap, we want to have a sense of their
non-separability. Using the operator $T$ we previously defined,  $\rho(T)$ is the
(positive) inner radius of $\mathcal{S}(T)$ . \cite{Allamigeon2018} , i.e. the supremum of
the radii of the Hilbert balls contained in the intersection between
$\mathcal{V}^{+}$ and $\mathcal{V}^{-}$. The next Proposition shows
us that this measure can be interpreted geometrically, in the case
where $\mathcal{V}^{+}$ and $\mathcal{V}^{-}$ are generated by point
clouds $X^{+}$ and $X^{-}$:
\begin{prop}
By moving some points from $X^{+}$ and $X^{-}$ by a distance of
at most $\rho(T)$, we can nullify the interior of the intersection
of the tropical spans.

More specifically, we note $a\in\mathbb{R}_{\max}^{d}$ the eigenvector
corresponding to $\rho(T)$. We project all points of $X^{+}$ and
$X^{-}$ whose distance to $\mathcal{H}_{a}$ is less than $\rho(T)$,
onto $\mathcal{H}_{a}$. Then the intersection of new convex hulls
is of empty interior.
\end{prop}

\begin{proof}
Let's denote $X$ the cloud consisting of all points (regardless of
sign), and for $x_{j}\in X$, we note $s_{j}$
its sector and $d_{j}$ the second argmax of $(x_{j}-a)$. For each
point $x_{j}=X_{\cdot j}\in X$ at distance less than $\rho(T)$ of
$\mathcal{H}_{a}$, the transformation consists in setting 
\[
W_{kj}:=\begin{cases}
X_{kj} & \text{if \ensuremath{k\ne s_{j}}}\\
X_{s_{j}j}-d_H(x_{j},\mathcal{H}_{a}) & \text{at \ensuremath{s_{j}}}
\end{cases}
\]
so that $w_{j}$ is projected on the hyperplane. As $T^{\pm}$ is
non-expansive and diagonal-free, let's remark that for $x^{\pm}\in V^{\pm}$:
\[
x_{i}^{\pm}\le T^{\pm}(x^{\pm})_{i}=\left(T^{\pm}(x^{\pm})-T^{\pm}(a)\right)_{i}+T^{\pm}(a)_{i},
\]
hence
\begin{equation}
x_{i}^{\pm}\le\max(x^{\pm}-a)_{\ne i}+T^{\pm}(a)_{i}.\label{eq31}
\end{equation}
Let $1\le i\le d$. If $x_{j}\in X$ is not in the $i$-th sector, then
for $k$ different from the sector of $x_{j}$, by definition: 
\[
(w_{j}-a)_{k}\le(x_{j}-a)_{d_{j}}\le(w_{j}-a)_{s_{j}},
\]
hence 
\[
W_{ij}-\max_{k\ne i}\left(W_{kj}-a_{k}\right)\le a_{i}.
\]
Otherwise, $x_{j}$ is in the $i$-th sector and: 
\[
\max_{k\ne i}\left(W_{kj}-a_{k}\right)=X_{d_{j}j}-a_{d_{j}},
\]
thus 
\[
W_{ij}-\max_{\ne i}\left(w_{j}-a\right)=\left(w_{j}-a\right)_{s_{j}}-\left(x_{j}-a\right)_{d_{j}}+a_{s_{j}}\ge a_{s_{j}}=a_{i},
\]
with equality iff $d_H(x_{j},\mathcal{H}_{a})\leq\rho(T)$.

Suppose by symmetry that $T(a)_{i}=T^{+}(a)_{i}=\rho(T)+a_{i}.$ We
also have $T^{-}(a)_{i}\ge\rho(T)+a_{i}$. Then, using the proof of
Theorem 22 in \cite{Akian2021TropicalLR}, we know that there exists
$j^{+},j^{-}\in[p]$ such that $x_{j^{+}}\in X^{+}$ and $x_{j^{-}}\in X^{-}$
are in sector $i$, with $x_{j^{+}}$ being at distance $\rho(T)$
from $\mathcal{H}_{a}$ and $x_{j^{-}}$ at distance greater than $\rho(T)$.
Therefore, $W_{ij^{+}}-\max_{\ne i}\left(w_{j^{+}}-a\right)=a_{i}$
and $W_{ij^{-}}-\max_{\ne i}\left(w_{j^{-}}-a\right)\geq a_{i}$.
Moreover, for any $j$ such that $x_{j}\in X^{+}$ is in sector $i$,
eq. \ref{eq31} gives $d_H(x_{j},\mathcal{H}_{a})\leq\rho(T)$.

Let $Q^{\pm}$ be the \emph{diagonal-free} projections over transformed
point clouds, and $Q=\inf(Q^{+}, Q^{-}).$ We've just shown that $Q(a)_{i}=Q^{+}(a)_{i}=a_{i}$,
and finally $Q(a)=a$.
\end{proof}
%
Thus, the spectral radius can be interpreted as a bound on the distance
under which some points have to be moved separate the interiors of convex hulls.

As $a$ is the center of the inner ball of $\mathcal{V}^{+}\cap\mathcal{V}^{-}$,
we also have a purely geometric heuristic for determining a
hyperplane in the non-separable case: we simply take the hyperplane
$\mathcal{H}_{a}$ and then assign the sectors based on dominant
population, for instance.


\subsection{Hard-margin multiclass classification}

In this section, we consider convex hulls of point clouds of $n$
classes, noted $\mathcal{V}^{k}$ for $1\le k \le n$, and described by Shapley operators
$T^{k}$. We give a simple criteria for these classes to be tropically separable in their whole, meaning that there exists a tropical signed
hyperplane such that each $\mathcal{V}^{k}$ belong to sectors of $I^{k}\subset\{1,\ldots, d\}$
with $I^{k}\cap I^{l}=\emptyset$ for $k\ne l\in\{1, \ldots, n\}.$ We then adapt
the previous results to this case.

We now consider the Shapley operator
\[
T:=\sup_{1\le k<l\le n}\inf(T^{k}, T^{l}).
\]

Morally, we can think of the union of 2 to 2 intersections of data classes, although the sup operator does not strictly speaking correspond to a union. We also remark that when $n=2$, $T$ is the same as previously
defined.

Let $(a,\rho(T))$ the eigenpair of $T$ approximated by the Kranoselskii-Mann
algorithm, verifying $T(a)=\rho(T)+a$. We define the sectors:
\[
I^{k}:=\{1\le i\le d,\quad T^{k}(a)_{i}>\rho(T)+a_{i}\}.
\]

Given the definition of $T$, it is clear that $I^{k}\cap I^{l}=\emptyset$
for $1\le k\ne l\le n,$ and we have the following result :
\begin{prop}
If $\rho(T)<0$, the tropical parametrized hyperplane $\mathcal{H}_{a}^{\sigma}$,
given the configuration $\sigma=\{I^{k}\}_{1\le k\le n}$, separates $\mathcal{V}^{k}$
and $\mathcal{V}^{l}$ for all $1 \le k\ne l \le n$ with a margin of $-\rho(T)$.
Moreover, this margin is optimal in the case where all $T^{k}$ are
of the form
\[
T^{k}(x)=P_{\mathcal{V}^{k}}(x)=\sup_{v\in \mathcal{V}^{k}}\left(v_{i}+\min(-v+x)\right),
\]
 which is in particular the case when separating finite point clouds. 
\end{prop}

\begin{proof}
Let $k\in\{1,\ldots, n\}$ and $i\in\bigsqcup_{\ell\ne k}I^{\ell}$. Using the same reasoning
as in the proof of Proposition $19$, we obtain that for all $x^{k}\in V^{k}$,
\[
d_H(\mathcal{H}_{a}^{\sigma},x^{k})=\max(x^{k}-a)-\max(x^{k}-a)_{\sqcup_{\ell\ne k}I^{\ell}}\ge-\lambda,
\]
hence the margin. Let's now prove the optimality in the case where
all $T^{k}$ are of the aforementioned form. For all $1\le i\le d$, there are two distinct classes $k\ne l$
such that $\inf(T^{k}(a)_{i}, T^{l}(a)_{i})=\rho(T)+a_{i}$. As $I^{k}\cap I^{l}=\emptyset$,
we can suppose by symmetry that $i\in\bigsqcup_{\ell\ne k}I^{\ell}.$ Then using
the same argument as in the proof of optimality in Proposition \ref{TODO:CITE},
we know that for all $\varepsilon>0$ there is a point $v^{k}\in V^{k}$
such that $\max(v^{k}-a)-(v_{i}^{k}-a_{i})\le-\rho(T)+\varepsilon$,
which means that 
\[
d_H(\mathcal{H}_{a}^{\sigma},x^{k})=\max(x^{k}-a)-\max(x^{k}-a)_{\sqcup_{\ell\ne k}I^{\ell}}\le-\lambda+\varepsilon.
\]

As this holds for every sector $i\in\{1,\ldots, d\}$, we have proven the optimality. 
\end{proof}

\section{Tropical Polynomials as Piecewise Linear Classifiers}

In this section, we extend our previous approach to piecewise linear
fitting, using tropical polynomials. 

A \emph{tropical polynomial} \emph{function} over $\mathbb{R}_{\max}^{d}$
is defined by
\[
f(x)=\bigoplus_{\alpha\in\mathbb{Z}^{d}}f_{\alpha}x^{\alpha}=\max_{\alpha\in\mathbb{Z}^{d}}\left(f_{\alpha}+\langle x,\alpha\rangle\right),
\]
where $f_{\alpha}=-\infty$ except for finitely many values of $\alpha$,
and with $\langle x,\alpha\rangle=\sum_{i}x_{i}\cdot\alpha_{i}$ under
the convention $(-\infty)\cdot0=0$.

The \emph{tropical hypersurface} $V(f)$ is the set of points from
$\mathbb{R}_{\max}^{d}$ where the maximum in $f$ is attained twice.
Tropical hypersurfaces are piecewise linear and divide $\mathbb{R}_{\max}^{d}$
between sectors corresponding to the maximal monomial: they can perform
more complex classification.

If $\mathcal{A}\subset\mathbb{Z}^{d}$ is a set of vectors, we define
the \emph{Veronese embedding} of $x$ as:
\[
\text{ver}_{\mathcal{A}}(x):=\left(\langle x,\alpha\rangle\right)_{\alpha\in\mathcal{A}}\in\mathbb{R}_{\max}^{\mathcal{A}},
\]
allowing us to map our point space into a larger space, made up of
various integer combinations of features. To take a fairly exhaustive
subset of monomials, we consider the integer combinations defined
by the integer points of the dilated simplex. Noting $s\in\mathbb{N}^{*}$
a scale parameter, we define
\[
\mathcal{A}_{s}:=\left\{x\in\mathbb{N}^{d},\quad x_{1}+\cdots+x_{d}=s\right\},
\]
which grows polynomially with $s$. The Veronese embedding defined
by $\mathcal{A}_{s}$ is invertible and if $\mathcal{H}$ is a tropical
hyperplane in $\mathbb{R}_{\max}^{\mathcal{A}}$, $\text{ver}_{\mathcal{A}_{s}}^{-1}(\mathcal{H})$
is a tropical polynomial hypersurface in the initial space (proof
/ quote?). Hence, $\text{ver}_{\mathcal{A}_{s}}$ is a feature map
enabling us to perform piecewise linear classification in $\mathbb{R}_{\max}^{d}$. This brings us close to the framework of neural networks with piecewise linear activations, e.g. relaxed linear units \cite{zhang2018tropical}. We remark that $\mathcal{A}_1$ corresponds to tropical hyperplanes: we are generalizing the approach developed in Section \ref{sec2}.

\begin{prop}
Margin theorem: TBD
\end{prop}
\begin{proof}
We want to link the Hilbert norm between initial space and Veronese-augmented
space $\mathbb{R}^{\mathcal{A}}$, using linear transfromation $V$.
We note $\mathcal{H}^{\mathcal{A}}$ the hypersurface in augmented
space, $\mathcal{H}:=V^{-1}(\mathcal{H}^{\mathcal{A}})$ the corresponding
tropical hypersurface, $Y\in\mathbb{R}_{\max}^{d}$ from the initial
space, and $y:=VY$. Using the linearity of $V$, controlling the
link between the norms from one side amounts to showing 
\[
\lVert Y\rVert_{H}\ge C\lVert VY\rVert_{H},
\]
Using the lemma 
\[
\lVert Y\rVert_{H}=2\inf_{\alpha\in\mathbb{R}}\lVert Y+\alpha e\rVert_{\infty},
\]
we only need to show that 
\[
\lVert Y\rVert_{\infty}\ge C\lVert VY\rVert_{\infty}.
\]
for some constant $C$. Indeed, for $\alpha=0$, it amounts to $\lVert VY\rVert_{H}\le2\lVert VY\rVert_{\infty}$,
hence $\lVert Y\rVert_{\infty}\ge\frac{C}{2}\lVert VY\rVert_{H}$.
Then, if $C$ is independent from $Y$, then we also have $\lVert Y+\alpha e\rVert_{\infty}\ge C\lVert VY+\frac{\alpha}{s}Ve\rVert_{\infty}=C\left\lVert V\left(Y+\frac{\alpha}{s}e\right)\right\rVert_{\infty}$,
as $Ve_{n}=se_{n}$ because $\sum\alpha_{i}=s$. Then for all $\alpha$,
\[
\lVert Y+\alpha e\rVert_{\infty}\ge\frac{C}{2}\lVert V(Y+\alpha e)\rVert_{H}=\frac{C}{2}\lVert VY+s\alpha e\lVert_{H}=\frac{C}{2}\lVert VY\lVert_{H}.
\]
Taking the infimum yields $\frac{1}{2}\lVert Y\rVert_{H}\ge\frac{C}{2}\lVert VY\rVert_{H},$
hence
\[
\lVert Y\rVert_{H}\ge C\lVert VY\rVert_{H}.
\]

Let's find constant $C$. By definition,
\[
\lVert VY\rVert_{\infty} =\sup_{k}\left|\sum_{j}V_{kj}Y_{j}\right|
 \le\sup_{k}\sum_{j}V_{kj}\cdot\lVert Y\rVert_{\infty}\\
 =s\lVert Y\rVert_{\infty},
\]
because $Ve_{n}=se_{n}$, so we define $C=1/s$.

Reciprocally, we only need to show that 
\[
\lVert Y\rVert_{\infty}\le C'\lVert VY\rVert_{\infty}
\]
using some constant $C'$. Denoting $y=VY$, we have $V^{T}y=V^{T}VY$,
where $V^{T}V$ is definite positive. Hence, $Y=(V^{T}V)^{-1}V^{T}y$,
and
\[
\lVert Y\rVert_{\infty}=\lVert(V^{T}V)^{-1}V^{T}y\rVert_{\infty}\le\lVert(V^{T}V)^{-1}V^{T}\rVert_{\text{op},\infty}\lVert VY\rVert_{\infty}
\]
hence we define $C'=\lVert(V^{T}V)^{-1}V^{T}\rVert_{\text{op},\infty}$.

\paragraph{Exploration.}

$V$ is a $\ell_{s,d}\times n$ matrix, where $\ell_{s,d}$ is the cardinal
of $\mathcal{A}^{s}$ and satisfies the equation
\[
\ell_{s,d}=\ell_{s,d-1}+\ell_{s-1,d},
\]
because when taking a point from $\mathcal{A}^{s}$, either the first
coordinate is equal to $0$ and there are $\ell_{s,d-1}$ possibilities
for the rest, either the first coordinate is nonzero, so that removing
$1$ to it and getting a point of $\mathcal{A}^{s-1}$ is a bijective
transformation. $\ell_{n,n}=\binom{2n}{n}-\binom{2(n-1)}{n-1}$ corresponds
to OEIS \#A051924.

Rows of $V$ are points from the external face of the dilated simplex.
As the latter is invariant by permutating axes, we have
\[
V^{T}V=(V_{\cdot i}^{T}V_{\cdot j})_{ij}=\begin{pmatrix}\alpha+\beta &  & \beta\\
 & \ddots\\
\beta &  & \alpha+\beta
\end{pmatrix}=\alpha I+\beta H,
\]
where $\alpha+\beta=\lVert V_{\cdot1}\rVert^{2}$, $\beta=\langle V_{\cdot1},V_{\cdot2}\rangle$
and $H=\begin{pmatrix}1 & \cdots & 1\\
\vdots & \ddots & \vdots\\
1 & \cdots & 1
\end{pmatrix}=ee^{T}$. Given the lemma 6.8 from the MAP557 course, we have
\[
(V^{T}V)^{-1}=\alpha^{-1}I-\frac{\beta}{\alpha^{2}(1+n\alpha^{-1}\beta)}H.
\]
And:
\[\alpha+\beta=\lVert V_{\cdot1}\rVert^{2}=\sum_{k=0}^{s}k^{2}\ell_{s-k,d-1}\]
\[\alpha=\sum_{k=0}^{s}\sum_{j=0}^{s-k}\ell_{s-j-k,d-2}\cdot k(k-j)\]
\[\beta=\langle V_{\cdot1},V_{\cdot2}\rangle=\sum_{k=0}^{s}\sum_{j=0}^{s-k}\ell_{s-j-k,d-2}\cdot kj\]

Lastly, as $V^{T}H=s\mathcal{H}_{\ell_{s,d}\times n}:=sH'$,
\[
\lVert(V^{T}V)^{-1}V^{T}\rVert_{\text{op},\infty}=\left\lVert\alpha^{-1}V^{T}-\frac{\beta s}{\alpha^{2}(1+n\alpha^{-1}\beta)}H'\right\rVert_{\text{op},\infty},
\]
which (for the moment) seems very hard to control or compute!

\paragraph{Remarks}
\begin{itemize}
\item $V^{T}V$ is not diagonally dominant, $V$ neither
\item $\lambda$ (eigenvalue from inner radius) grows linearly with $d$
\item Setting $s$ beforehand, $\lVert(V^{T}V)^{-1}V^{T}\rVert_{\text{op},\infty}$
seems to converge in $d$ to some reasonable value in $\propto1-e^{-d}$
(to give an idea)
\item Setting $d$ beforehand, $\lVert(V^{T}V)^{-1}V^{T}\rVert_{\text{op},\infty}$
grows with $s$ in $\ln s$ (to give an idea: converges or not?)
\item $\ell_{n,n} \sim \frac{3\cdot 4^{n}}{4 \sqrt{\pi n}}$ 
\item en gros de gros $\ell_{s,d} \sim \frac{3\cdot 2^{s+d}}{4 \sqrt{\pi}(sd)^{1/4}}$ 
\end{itemize}
\end{proof}

In Figures \ref{fig:iris} and \ref{fig:toy}, we visualize the decision boundaries of the piecewise linear classifier depending on scale parameter $s$. Dotted lines in light gray represent merged sectors after assignment. In Figure \ref{fig:iris}, we selected $3$ features from the Iris In Figure \ref{fig:toy}, we developed a toy 2D dataset with $4$ different classes. $s=3$ seems to better classify our toy data classes, whereas $s=4$ seems more prone to overfitting. $s$ is a hyperparameter describing the complexity of our piecewise linear models.

\begin{figure}[!h]
    \centering
    \resizebox{0.7\textwidth}{!}{\input{fig/A_bis__iris_3.pgf}}
    \caption{Projected Iris dataset, $s=3$}
    \label{fig:iris}
\end{figure}

\begin{figure}[!h]
    \centering
    \resizebox{\textwidth}{!}{\input{fig/A_bis__toy_4.pgf}}
    \caption{Toy dataset, $s = 1\ldots 4$}
    \label{fig:toy}
\end{figure}

\newpage
\subsection{Linear Hyperplanes Look Tropical on Log Paper}

Let $X_{ij}$ be our point clouds, and for $\beta>0$, let's define
$x^{\beta}=(x_{ij}^{\beta}:=e^{\beta X_{ij}})_{ij}$. We will show
that separating different classes from $x^{\beta}$ using a linear
SVM, when $\beta$ tends toward infinity, yields a separating hyperplane
that converges towards a tropical hyperplane in the initial space.

Our method, by directly outputting an optimal-margin separating tropical
hyperplane in pseudo-polynomial time, is expected to achieve better
results. If $\beta$ is small, we might indeed be far away from the
limiting tropical hypersurface; conversely, as $\beta$ approaches
infinity, numerical error becomes predominant.

\begin{figure}[h]
\centering \includegraphics[scale=0.1]{\string"fig/linear log-exp kernel\string".png}
\includegraphics[scale=0.1]{\string"fig/tropical svm\string".png} 
\end{figure}


\subsubsection{Tropical hyperplanes are limiting classical hyperplanes}

Under the assumption that the $x_{ij}^{\beta}$ are linearly separable
\textbf{\emph{(strong assumption because it has to hold for all values
of $\beta$)}}, we compute a support vector classifier without intercept
term, whose separation surface's equation is: 
\[
H^{\beta}:\quad w^{\beta}\cdot x=0,
\]
where all positive (resp. negative) points verify $w^{\beta}\cdot x\ge1$
(resp. $w^{\beta}\cdot x\le-1$). We write 
\[
w_{i}^{\beta}=\sigma_{i}e^{\beta W_{i}^{\beta}},
\]
where $\sigma_{i}\in\{\pm1\}$ and $W_{i}^{\beta}\in\mathbb{R}\cup\{-\infty\}$.
For now, we simplify the study by considering a fixed $W_{i}$ value
and $w_{i}^{\beta}=\sigma_{i}e^{\beta W_{i}}$. 
\begin{lem}
(Maslov's sandwich) For $\beta>0$ and $I\subset[d]$ we have: 
\[
0\leq\beta^{-1}\log\left(\sum_{i\in I}e^{\beta(W_{i}+X_{ij})}\right)-\max_{i\in I}(W_{i}+X_{ij})\le\beta^{-1}\log d.
\]
\end{lem}

\begin{proof}
Let $\beta>0$ and $I\subset[d]$. We have: 
\[
\exp\left\{ \beta\max_{i\in I}\left(W_{i}+X_{ij}\right)\right\} \le\sum_{i\in I}e^{\beta(W_{i}+X_{ij})}\le d\cdot\exp\left\{ \beta\max_{i\in I}\left(W_{i}+X_{ij}\right)\right\} ,
\]
hence the result by taking the logarithm and dividing by $\beta$. 
\end{proof}
\begin{prop} 

Defining $H^{\text{trop}}$ as the hyperplane of apex $(-W_{i})_{i\in[d]}$,
signed using $I^{+}:=\{i\in[d],\quad\sigma_{i}>0\}$ and $I^{-}:=[d]\backslash I^{+}$,
we have: 
\[
d_{H}\left(\log H^{\beta},H^{\text{trop}}\right)\le\beta^{-1}\log d,
\]
where $d_{H}$ is the tropical Haussdorf distance. Hence $\log H^{\beta}\longrightarrow H^{\text{trop}}$
as $\beta\longrightarrow+\infty$. 
\end{prop}

\begin{proof}
Let $\beta>0$ and $X\in H^{\beta}$. By writing the inequalities
from previous lemma with $I^{+}$ and $I^{-}$, and as $$\beta^{-1}\log\left(\sum_{i\in I^{+}}e^{\beta(W_{i}+X_{ij})}\right)=\beta^{-1}\log\left(\sum_{i\in I^{-}}e^{\beta(W_{i}+X_{ij})}\right),$$
substracting the first to second inequality yields: 
\[
-\beta^{-1}\log d\le\max_{i\in I^{+}}(W_{i}+X_{ij})-\max_{i\in I^{-}}(W_{i}+X_{ij})\le\beta^{-1}\log d.
\]
hence $d_H(X,H^{\text{trop}})\le\beta^{-1}\log d$.

Reciprocally, let $Y\in H^{\text{trop}}$ and $X=Y+\delta\mathbf{1}_{I^{+}}$,
with $\delta$ to be defined later. To have $Y\in H^{\beta}$, we
have to ensure that: 
\[
w^{\beta}\cdot y=0,
\]
which amounts to 
\[
\sum_{i=1}^d\sigma_{i}e^{\beta W_{i}}e^{\beta X_{i}}=0.
\]
By separating the positive and negative terms, and taking the logarithm,
we get 
\[
\beta\delta+\log\left(\sum_{i\in I^{+}}e^{\beta(W_{i}+X_{ij})}\right)=\log\left(\sum_{i\in I^{-}}e^{\beta(W_{i}+X_{ij})}\right),
\]
which similarily yields 
\[
\delta\le\left|\beta^{-1}\log\left(\sum_{i\in I^{+}}e^{\beta(W_{i}+X_{ij})}\right)-\beta^{-1}\log\left(\sum_{i\in I^{-}}e^{\beta(W_{i}+X_{ij})}\right)\right|\le\beta^{-1}\log d,
\]
hence $d_H(Y,H^{\beta})\le\beta^{-1}\log d$. 
\end{proof}
\begin{rem}
We note $L$ the order of magnitude of our data points, and $B$ a
typical number at which the computer starts having numerical errors
or overflow (typically, for C integer calculations, $B=2^{16}-1)$.
We want to have $\beta L\ll\log B$ (that is, no numerical error),
and $\beta^{-1}\log d\ll L$ (good convergence towards tropical hyperplane),
i.e $d\ll B$. By directly computing logarithms, for instance, our
method should be very suitable for $d\apprge65535$ dimensions using
C integers. 
\end{rem}


\subsubsection{Finding tropical apex}

In the classical hard-margin setting, admissible $w$ vectors verify
$w^{T}x^{+}\ge1$ (resp. $w^{T}x^{-}\le-1$) for positive (resp. negative)
vectors. Thus they belong to the polytope $P^{\beta}$ where 
\[
P^{\beta}:=\left\{ w\in\mathbb{R}^{d},\quad w^{T}x_{j_{+}}^{\beta}\ge1\text{ and }w^{T}x_{j_{-}}^{\beta}\le-1,\forall j_{+},j_{-}\in J^{+},J^{-}\right\} .
\]

We hope that $L_{\sigma}(P^{\beta}):=\left\{ (\beta^{-1}\log(\sigma_{i}w_{i}))_{1\le i\le d}\right\} $
converges towards the corresponding limiting tropical polytope $P_{\sigma}^{\text{trop}}:=P_{\sigma,+}^{\text{trop}}\cap P_{\sigma,-}^{\text{trop}}$,
where 
\[
P_{+,\sigma}^{\text{trop}}:=\left\{ W\in(\mathbb{R}\cup\{-\infty\})^{d},\quad\max_{i,\sigma_{i}=1}(W_{i}+X_{ij}^{\sigma})\ge\max_{i,\sigma_{i}=-1}(W_{i}+X_{ij}^{\sigma})\vee0,\forall j\in J^{+}\right\} .
\]
and 
\[
P_{-,\sigma}^{\text{trop}}:=\left\{ W\in(\mathbb{R}\cup\{-\infty\})^{d},\quad\max_{i,\sigma_{i}=-1}(W_{i}+X_{ij}^{\sigma})\le\max_{i,\sigma_{i}=1}(W_{i}+X_{ij}^{\sigma})\vee0,\forall j\in J^{-}\right\} .
\]

\begin{thm}
(Cite the corresponding article) When points $x_{ij}^{\beta}$ are
in a general position, \textbf{(clarify what this means as $\beta$}
\textbf{approaches infinity)} 
\[
\lim_{\beta\rightarrow+\infty}L_{\sigma}(P^{\beta})=P_{\sigma}^{\text{trop}},
\]
with respect to the Haussdorf distance. 
\end{thm}

We proved the convergence of the logarithm of linear hypersurfaces
towards a tropical limiting hypersurface, when the apex is fixed.
However, in practice, there is an underlying double limit here: for
each $\beta$ value, we compute an apex in the exponentialized space
and we hope it will converge towards a fixed apex in the initial space:
let's consider $w_{i}^{\beta}=\sigma_{i}e^{\beta W_{i}^{\beta}}$
again. 
\begin{thm}
(Cite the corresponding article) 
\[
W^{\infty}:=\lim_{\beta\rightarrow+\infty}\beta^{-1}\log w^{\beta}\in\underset{W\in P^{\text{trop}}}{\arg\min}\left(\max w\right).
\]
\end{thm}


\newpage{} \bibliographystyle{plain}
\bibliography{ea}

\end{document}
