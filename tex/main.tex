\documentclass[oneside,english,a4paper]{amsart}
\usepackage{bbold}
\usepackage[T1]{fontenc}
\usepackage{inputenc}
\setlength{\parskip}{\smallskipamount}
\setlength{\parindent}{0pt}
\usepackage{amstext}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{pgfplots}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{wasysym}
\usepackage{fullpage}
\usepackage{adjustbox}

\makeatletter
\numberwithin{equation}{section}
\numberwithin{figure}{section}
\theoremstyle{plain}
\newtheorem{thm}{\protect\theoremname}
\theoremstyle{definition}
\newtheorem{defn}[thm]{\protect\definitionname}
\theoremstyle{plain}
\newtheorem{prop}[thm]{\protect\propositionname}
\theoremstyle{remark}
\newtheorem{rem}[thm]{\protect\remarkname}
\theoremstyle{plain}
\newtheorem{lem}[thm]{\protect\lemmaname}
\theoremstyle{definition}
\newtheorem{example}[thm]{\protect\examplename}
\newtheorem{cor}[thm]{\protect\corollaryname}
\theoremstyle{definition}

\makeatother

\usepackage{babel}
\providecommand{\definitionname}{Definition}
\providecommand{\lemmaname}{Lemma}
\providecommand{\propositionname}{Proposition}
\providecommand{\remarkname}{Remark}
\providecommand{\theoremname}{Theorem}
\providecommand{\examplename}{Example}
\providecommand{\corollaryname}{Corollary}

\begin{document}
\title{Tropical Support Vector Machines and mean payoff games}
\author{Xavier Allamigeon, Stéphane Gaubert, Samuel Boïté, Théo Molfessis}
\maketitle

\begin{abstract}
    We develop new algorithms around tropical support vector machines (SVMs) for binary and multi-classification. We explore the use of tropical hyperplanes to partition tropical space, laying the groundwork for our classification approach. We address the challenge of separating overlapping data in tropical space, proposing a method to measure and handle data overlap using non-expansive Shapley operators and a Krasnoselskii-Mann iteration scheme. For separable data, we extend our method to determine hard-margin tropical hyperplanes, ensuring maximal separation. This is further applied to multi-classification scenarios, providing conditions for tropical separability and demonstrating margin optimality. Additionally, the tropical kernel trick is explored as a means to embed data into a higher-dimensional tropical space, transforming decision boundaries into tropical polynomials. This approach is empirically tested on several classic datasets. Finally, we show that tropical hyperplanes emerge as limiting cases of classical hyperplanes on logarithmic paper, making our approach more stable numerically.
\end{abstract}

\section{Introduction}

% TODO: add a "motivation & context" subsection to explain the relevance of the topic

\subsection*{The tropical linear classification problem}

The tropical \emph{max-plus} semifield $\mathbb{R}_{\max}$ is the
set of real numbers, completed by $-\infty$ and equipped with the
addition $a\oplus b=\max(a,b)$ and the multiplication $a\odot b=a+b$.
A \emph{tropical hyperplane} in the $d$-dimensional tropical vector
space $\mathbb{R}_{\max}^{d}$ is a set of vectors of the form
\[
\mathcal{H}_{a}=\left\{x\in\mathbb{R}_{\max}^{d},\quad\max_{1\le i\le d}(a_{i}+x_{i})\,\text{is achieved at least twice}\right\}.
\]
Such a hyperplane is parametrized by the vector $a=(a_{1},\ldots,a_{d})\in\mathbb{R}_{\max}^{d}$,
which is required to be non-identically $-\infty$. It hence partitions
the space depending on which coordinate reaches its maximum, laying the
ground for $d$-class classification. By definition, they are invariant by translation along the constant vector $(1,\ldots, 1)$.

In this paper, we address the following tropical analogue of support
vector machines. Given $n\in\{1,\ldots, d\}$ classes of $d$-dimensional data
points $X^{1},\ldots,X^{p}$, we look for a maximum-margin separating
tropical hyperplane to build a classifier. The notion of margin depends
on the metric: the canonical choice in tropical geometry is the (additive
version of) Hilbert's projective metric. Its restriction to $\mathbb{R}^{d}$
is induced by the so-called \emph{Hilbert's seminorm} or \emph{Hopf
oscillation}
\[
\lVert x\rVert_{H}:=\max_{1\le i\le d}x_{i}-\min_{1\le i\le d}x_{i}.
\]
It is a projective metric, in the sense that the distance between
two points is zero if and only if these two points differ by an additive
constant. When dealing with hard-margin support vector machines (SVMs),
we also have to ensure that every point lands in the right sector. Each class $k$ belongs to some sectors $I^k\subset \{1,\ldots, d\}$, with each sector assigned to only one class. We then define the \emph{tropical parametrized hyperplane} of configuration
$\sigma=\left\{I^{1},\ldots,I^{n}\right\}$, where $I^{k}$ and $I^{\ell}$
are disjoint for $k\ne\ell$, as:
\[
\mathcal{H}=\left\{x\in\mathbb{R}_{\max}^{d},\quad\exists k\ne\ell,\quad (x-a)\,\text{reaches its max coordinate in \ensuremath{I^{k}} and \ensuremath{I^{\ell}}}\right\}.
\]
We note $\mathcal{S}^k\subset \mathbb{R}_{\max}^d$ the \emph{sector} classified as class $k$, where the maximum coordinate is reached in $I^k$. $\mathcal{H}$ is said to \emph{separate} point
clouds $(X^{k})_{1\le k\le p}$ with a margin of $\nu$ when for every point
$x^{k}$ of class $k$:
\begin{enumerate}
\item $x^{k}$ is on the right sector of the hyperplane, i.e its maximum
coordinate is reached in $I^{k}$
\item $x^{k}$ is at distance at least $\nu$ from any other sector $\mathcal{S}^\ell$, $l \ne k$: %$\mathcal{H}_{a}^{\sigma}$:
\begin{equation*}
d_H(\mathcal{S}^\ell,x^{k})=\max(x^{k}-a)-\max_{i\in I^\ell}(x^{k}-a)_i\ge\nu.
\end{equation*}
\end{enumerate}

In Figure \ref{fig:MaxMargin}, we separate two classes
of $3$-dimensional points from a toy tropical dataset using a tropical
hyperplane. The two lower sectors are assigned to one class, the upper to the other. Its margin is maximal and is represented by the Hilbert
ball in gray. The figure is represented in the projective space $\mathbb{P}\left(\mathbb{R}_{\text{max}}^{3}\right)$,
i.e. the quotient of the set of non-identically $-\infty$ vectors
of $\mathbb{R}_{\text{max}}^{3}$ by the equivalence relation which
identifies tropically proportional values.

Since our separator is translation-invariant by the constant vector, in order to separate any $d$-dimensional dataset, we plunge it into the $x_1+\cdots+x_{d+1}=0$ subspace of $\mathbb{R}_{\max}^{d+1}$. This amounts to artificially adding a new coordinate to each point, equal to the opposite of the sum of the initial coordinates. In the rare case where we are dealing with an intrinsically tropical dataset, for which this translation invariance makes sense, it is better to apply the tropical classifier on the initial features.

\begin{figure}[!h]
    \centering
    \begin{subfigure}{0.55\textwidth}
        \centering
        \resizebox{0.5\textwidth}{!}{%
        \centering
    \clipbox{0.35\width{} 0.25\height{} 0.25\width{} 0.25\height{}}{\input{figures/bintoy-separated_1.pgf}}
}
        \caption{Maximum-margin tropical hyperplane}
        \label{fig:MaxMargin}
    \end{subfigure}
    \hfill
    \centering
    \begin{subfigure}{0.4\textwidth}
        \centering
        \resizebox{\linewidth}{!}{%
        \centering
            \clipbox{0.2\width{} 0.3\height{} 0.2\width{} 0.25\height{}}{\input{figures/iris_3.pgf}}
        }
        \caption{Iris dataset, sepal and petal width, cubic}
        \label{fig:iris}
    \end{subfigure}

    \bigskip
    \centering
    \begin{subfigure}{0.45\textwidth}
        \centering
        \resizebox{\linewidth}{!}{%
        \centering
            \clipbox{0.25\width{} 0.3\height{} 0.25\width{} 0.3\height{}}{\input{figures/circular_3.pgf}}
        }
        \caption{Toy binary dataset, cubic}
        \label{fig:circular}
    \end{subfigure}
    \hfill
    \centering
    \begin{subfigure}{0.35\textwidth}
        \centering
        \resizebox{\linewidth}{!}{%
        \centering
            \clipbox{0.2\width{} 0.2\height{} 0.2\width{} 0.2\height{}}{\input{figures/toy-reverse_3.pgf}}
        }
        \caption{Toy multiclass dataset, cubic}
        \label{fig:toy_reverse}
    \end{subfigure}
    
    \caption{Tropical piecewise linear classifiers}
    \label{fig:plots}
\end{figure}

\subsection*{Contribution}

We formulate the tropical classification problem as a mean-payoff game. We apply a Krasnoselskii-Mann iteration scheme to derive an $\epsilon$-approximation of the tropical classifier after ??? iterations. We prove the optimality of the margin in the separable case, and develop geometric guarantees in the opposite case. Finally, we propose a feature map for fitting tropical polynomials while retaining margin information. Thanks to their evaluation speed and the complexity of their piecewise linear hypersurfaces, we pave the way for complex classification tasks.

\newpage
\section{Tropical Support Vector Machines}\label{Sec2}

\subsection{Preliminaries on mean payoff games}

In this section, we have a finite set of points $X=(x_{1},\ldots,x_{p})\in\mathbb{R}_{\text{max}}^{d\times p}$.
We define the \emph{tropical span} of $X$ as the set of tropical
linear combinations of these points:
\[
\text{Span}(X):=\left\{\max_{1\le i\le p}(x_{i}+\lambda_{i}),\quad\lambda\in\mathbb{R}^{p}\right\}.
\]
As this is also the smallest tropically convex set containing these
points, separating several point clouds amounts to separating their
tropical spans. Given a tropically convex, compact and nonempty subset $\mathcal{V}$
of $\mathbb{R}_{\max}^{d}$, we define the projection of any vector
$x$ in $\mathbb{R}_{\max}^{d}$ by:
\[
P_{\mathcal{V}}(x):=\max\{y\in \mathcal{V},\quad y\le x\}.
\]
When $\mathcal{V}$ is the tropical span of $X$, we will also note
this projection as $P_{X}$. We know from Maclagan et al. \cite{Maclagan2015} that tropical projections
can be expressed as a mean payoff game: for $i\in[p]$ and $x\in\mathbb{R}_{\max}^{d}$,
\begin{equation}
P_{X}(x)_{i}=\max_{j\in[p]}\left\{X_{ij}+\min_{1\le k \le d}(-X_{kj}+x_{k})\right\}.\label{eq:projector}
\end{equation}

Here, we recognize the payoff of a zero-sum deterministic game with perfect
information. There are two players, ``Max'' and ``Min'' (the maximizer
and the minimizer), who alternate their actions. Starting on node
$i$, Player Max chooses to move to node $j$, and receives $X_{ij}$
from Player Min. Similarily, Player Min in turn chooses a node $k$
and has to pay $-X_{kj}$ to Player Max.

We now recall some tools from Perron-Frobenius theory, in relation
with mean payoff games. We refer the reader to \cite{AKIAN2012} for more
information. We call \emph{Shapley operator} a map $T:\mathbb{R}_{\max}^{d}\rightarrow\mathbb{R}_{\max}^{d}$
that is order preserving, continuous, and such that $T(\alpha+x)=\alpha+T(x)$
for all $\alpha\in\mathbb{R}_{\max}$ and $x\in\mathbb{R}_{\max}^{d}$. $T$ is said to be \emph{diagonal free} when $T_{i}(x)$
is independent of $x_{i}$ for all $i\in\{1,\ldots, n\}$, i.e. when for all $x,y\in\mathbb{R}_{\max}$ such that $x_{j}=y_{j}$
for all $j\neq i$, we have $T_{i}(x)=T_{i}(y)$. 

Observe that the tropical projection defined by \ref{eq:projector}
is a Shapley operator. Given a Shapley operator $T$, we also define
\[
\mathcal{S}(T):=\left\{x\in\mathbb{R}_{\max}^{d},\quad x\le T(x)\right\}.
\]
More specifically, any tropically convex set $\mathcal{V}$ is equal
to $\mathcal{S}(P_{\mathcal{V}})$. Indeed, as for any $x\in\mathbb{R}_{\text{max}}^{d}$,
$P_{\mathcal{V}}(x)\le x$, having $x\in\mathcal{S}(P_{\mathcal{V}})$
implies $x=P_{\mathcal{V}}(x)$, meaning $x$ is in
$\mathcal{V}$.


Operator $P$ can be slightly tweaked to make it \emph{diagonal-free} -- but the resulting operator isn't a projector anymore.
In the modified game, the opponent is prevented from replying eye-for-an-eye:
\[
\left[P^{\text{DF}}_X(x)\right]_{i}:=\max_{j\in[p]}\left\{ X_{ij}+\min_{k\ne i}(-X_{kj}+x_{k})\right\} .
\]

Noticeably, $\mathcal{S}(P_X^\text{DF}) = \mathcal{S}(P_X)$.

\subsection{Hard-margin binary SVM}

In this section, we introduce a practical criterion for the existence of a separating tropical hyperplane, and if it exists, we construct one with optimal margin. We also place ourselves in the binary setting, where we seek to two separate convex hulls $\mathcal{V}^{+}$
and $\mathcal{V}^{-}$. We assume we have two Shapley operators $T^{+}$
and $T^{-}$ such that $\mathcal{S}(T^{\pm})=\mathcal{V}^{\pm}$,
which is for instance the case when separating finite point clouds:
corresponding operators are projections $P_{\mathcal{V}^{\pm}}$. We define:
\[
T=\inf(T^{+},T^{-}),
\]
which is also a Shapley. We denote by $\perp$ the vector of $\mathbb{R}_{\max}^{d}$
identically equal to $-\infty$. The \emph{spectral radius} of $T$
is, by definition:
\[
\rho(T)=\sup\left\{\lambda\in\mathbb{R}_{\max},\quad\exists u\in\mathbb{R}_{\max}^{d}\backslash\{\perp\},\quad T(u)=\lambda+u\right\}.
\]

Observe that $\mathcal{S}(T)$ is equal to $\mathcal{S}(T^{+})\cap\mathcal{S}(T^{-})$,
i.e. the intersection of the convex hulls we are seeking to separate.
From Allamigeon, Gaubert et al. \cite{Allamigeon2018}, we know that $\mathcal{V}^{+}$
and $\mathcal{V^{-}}$ being disjoint is equivalent to $\rho(T)\le0$.

Given $a$ the eigenvector corresponding to $\rho(T)$, we define the sectors:
\[
I^{\pm}:=\left\{i\in\{1,\ldots, d\},\quad T^{\pm}(a)_{i}>\rho(T)+a_{i}\right\},
\]

TODO : RÉGLER QUESTION DE INF SUR LES SECTEURS ET ENVOI DE L'APEX A +INF

and the corresponding configuration $\sigma=\{I^{+},I^{-}\}$.
\begin{prop}
Hyperplane $\mathcal{H}_{a}^{\sigma}$, given the sectors defined
above, separates $\mathcal{V}^{+}$ and $\mathcal{V}^{-}$ with a
margin of $-\rho(T)$. Moreover, this margin is optimal when $T^{\pm}$
are of the form
\[
\sup_{v\in\mathcal{V}^{\pm}}\left(v_{i}+\min(-v+x)\right),
\]
which is in particular the case when separating finite point clouds.
\end{prop}

\begin{proof}
As $T^{\pm}$ is non-expansive, for $x^{\pm}\in V^{\pm}$:
\[
x_{i}^{\pm}\le T^{\pm}(x^{\pm})_{i}=\left(T^{\pm}(x^{\pm})-T^{\pm}(a)\right)_{i}+T^{\pm}(a)_{i},
\]
hence 
\begin{equation}
x_{i}^{\pm}\le\max(x^{\pm}-a)+T^{\pm}(a)_{i}.\label{eq41}
\end{equation}
For instance, let $i\in I^{-}$. Then $T^{+}(a)_{i}=\rho(T)+a_{i}$,
so for $x^{+}\in \mathcal{V}^{+}$, using equation \ref{eq41}:
\[
x_{i}^{+}-a_{i}\le\max(x^{+}-a)+\rho(T).
\]
In particular, $x_{i}^{+}-a_{i}<\max(x^{+}-a)$ and any element of
$\mathcal{V}^{+}$ cannot belong to any of sectors in $I^{-}$ with
respect to $\mathcal{H}_{a}$, from which the sectors $I^{\pm}$ are well-defined.
Finally, 
\[
d_H(\mathcal{S}^-,x^{+})=\max(x^{+}-a)-\max(x^{+}-a)_{I^{-}}\ge-\rho(T),
\]
hence the margin.

Let's finally prove that the margin is optimal in the case where $T^{\pm}$
are of the aforementioned form.
Let $i\in I^{-}$. Then, for $\varepsilon>0$, we can
find $v\in \mathcal{V}^{+}$ such that 
\[
T^{+}(a)_{i}-\varepsilon\le v_{i}-\max(v-a)\le T^{+}(a)_{i},
\]
giving us 
\[
\rho(T)-\varepsilon\le v_{i}-a_{i}-\max(v-a)\le\rho(T).
\]
Maximizing over all $i\in I^{-}$ gives that $v$ is
at most at distance $-\rho(T)+\varepsilon$ of $\mathcal{H}_{a}^{\sigma}$, hence
the optimality.
\end{proof}
%
To build a practical algorithm, we still need to compute the spectral
radius of $T$. As it amounts to solving mean-payoff games, we can apply the following Krasnoselskii-Mann iteration scheme,
given any initial point $a^{0}$:
\[
\begin{cases}
z^{k+1} & =\frac{1}{2}\left(a^{k}+T(a^{k})\right)\\
a^{k+1} & =z^{k+1}-\max_{1\le i\le d}z_{i}^{k+1}
\end{cases}
\]
The eigenpair we search is $(a^{\infty},2\cdot\max_{1\le i\le d}z_{i}^{\infty})$.
Example results of this algorithm on a toy dataset are shown in Figure
\ref{fig:MaxMargin}.

TODO : TALK ABOUT EFFICIENCY

\subsection{Geometric approach for soft-margin SVM}

We seek to separate convex hulls $\mathcal{V}^{+}$ and $\mathcal{V}^{-}$.
Assuming that the data overlap, we want to have a sense of their
non-separability. Using the operator $T$ we previously defined,  $\rho(T)$ is the
(positive) inner radius of $\mathcal{S}(T)$ . \cite{Allamigeon2018} , i.e. the supremum of
the radii of the Hilbert balls contained in the intersection between
$\mathcal{V}^{+}$ and $\mathcal{V}^{-}$. The next Proposition shows
us that this measure can be interpreted geometrically, in the case
where $\mathcal{V}^{+}$ and $\mathcal{V}^{-}$ are generated by point
clouds $X^{+}$ and $X^{-}$:
\begin{prop}
By moving some points from $X^{+}$ and $X^{-}$ by a distance of
at most $\rho(T)$, we can nullify the interior of the intersection
of the tropical spans.

More specifically, we note $a\in\mathbb{R}_{\max}^{d}$ the eigenvector
corresponding to $\rho(T)$. We project all points of $X^{+}$ and
$X^{-}$ whose distance to $\mathcal{H}_{a}$ is less than $\rho(T)$,
onto $\mathcal{H}_{a}$. Then the intersection of new convex hulls
is of empty interior.
\end{prop}

\begin{proof}
Let's denote $X$ the cloud consisting of all points (regardless of
sign), and for $x_{j}\in X$, we note $s_{j}$
its sector and $d_{j}$ the second argmax of $(x_{j}-a)$. For each
point $x_{j}=X_{\cdot j}\in X$ at distance less than $\rho(T)$ of
$\mathcal{H}_{a}$, the transformation consists in setting 
\[
W_{kj}:=\begin{cases}
X_{kj} & \text{if \ensuremath{k\ne s_{j}}}\\
X_{s_{j}j}-d_H(x_{j},\mathcal{H}_{a}) & \text{at \ensuremath{s_{j}}}
\end{cases}
\]
so that $w_{j}$ is projected on the hyperplane. As $T^{\pm}$ is
non-expansive and diagonal-free, let's remark that for $x^{\pm}\in V^{\pm}$:
\[
x_{i}^{\pm}\le T^{\pm}(x^{\pm})_{i}=\left(T^{\pm}(x^{\pm})-T^{\pm}(a)\right)_{i}+T^{\pm}(a)_{i},
\]
hence
\begin{equation}
x_{i}^{\pm}\le\max(x^{\pm}-a)_{\ne i}+T^{\pm}(a)_{i}.\label{eq31}
\end{equation}
Let $1\le i\le d$. If $x_{j}\in X$ is not in the $i$-th sector, then
for $k$ different from the sector of $x_{j}$, by definition: 
\[
(w_{j}-a)_{k}\le(x_{j}-a)_{d_{j}}\le(w_{j}-a)_{s_{j}},
\]
hence 
\[
W_{ij}-\max_{k\ne i}\left(W_{kj}-a_{k}\right)\le a_{i}.
\]
Otherwise, $x_{j}$ is in the $i$-th sector and: 
\[
\max_{k\ne i}\left(W_{kj}-a_{k}\right)=X_{d_{j}j}-a_{d_{j}},
\]
thus 
\[
W_{ij}-\max_{\ne i}\left(w_{j}-a\right)=\left(w_{j}-a\right)_{s_{j}}-\left(x_{j}-a\right)_{d_{j}}+a_{s_{j}}\ge a_{s_{j}}=a_{i},
\]
with equality iff $d_H(x_{j},\mathcal{H}_{a})\leq\rho(T)$.

Suppose by symmetry that $T(a)_{i}=T^{+}(a)_{i}=\rho(T)+a_{i}.$ We
also have $T^{-}(a)_{i}\ge\rho(T)+a_{i}$. Then, using the proof of
Theorem 22 in \cite{Akian2021TropicalLR}, we know that there exists
$j^{+},j^{-}\in[p]$ such that $x_{j^{+}}\in X^{+}$ and $x_{j^{-}}\in X^{-}$
are in sector $i$, with $x_{j^{+}}$ being at distance $\rho(T)$
from $\mathcal{H}_{a}$ and $x_{j^{-}}$ at distance greater than $\rho(T)$.
Therefore, $W_{ij^{+}}-\max_{\ne i}\left(w_{j^{+}}-a\right)=a_{i}$
and $W_{ij^{-}}-\max_{\ne i}\left(w_{j^{-}}-a\right)\geq a_{i}$.
Moreover, for any $j$ such that $x_{j}\in X^{+}$ is in sector $i$,
eq. \ref{eq31} gives $d_H(x_{j},\mathcal{H}_{a})\leq\rho(T)$.

Let $Q^{\pm}$ be the \emph{diagonal-free} projections over transformed
point clouds, and $Q=\inf(Q^{+}, Q^{-}).$ We've just shown that $Q(a)_{i}=Q^{+}(a)_{i}=a_{i}$,
and finally $Q(a)=a$.
\end{proof}
%
Thus, the spectral radius can be interpreted as a bound on the distance
under which some points have to be moved separate the interiors of convex hulls.

As $a$ is the center of the inner ball of $\mathcal{V}^{+}\cap\mathcal{V}^{-}$,
we also have a purely geometric heuristic for determining a
hyperplane in the non-separable case: we simply take the hyperplane
$\mathcal{H}_{a}$ and then assign the sectors based on dominant
population, for instance.


\subsection{Hard-margin multiclass classification}

In this section, we consider convex hulls of point clouds of $n$
classes, noted $\mathcal{V}^{k}$ for $1\le k \le n$, and described by Shapley operators
$T^{k}$. We give a simple criteria for these classes to be tropically separable in their whole, meaning that there exists a tropical signed
hyperplane such that each $\mathcal{V}^{k}$ belong to sectors of $I^{k}\subset\{1,\ldots, d\}$
with $I^{k}\cap I^{l}=\emptyset$ for $k\ne l\in\{1, \ldots, n\}.$ We then adapt
the previous results to this case.

We now consider the Shapley operator
\[
T:=\sup_{1\le k<l\le n}\inf(T^{k}, T^{l}).
\]

Morally, we can think of the union of 2 to 2 intersections of data classes, although the sup operator does not strictly speaking correspond to a union. We also remark that when $n=2$, $T$ is the same as previously
defined.

Let $(a,\rho(T))$ the eigenpair of $T$ approximated by the Kranoselskii-Mann
algorithm, verifying $T(a)=\rho(T)+a$. We define the sectors:
\[
I^{k}:=\{1\le i\le d,\quad T^{k}(a)_{i}>\rho(T)+a_{i}\}.
\]

Given the definition of $T$, it is clear that $I^{k}\cap I^{l}=\emptyset$
for $1\le k\ne l\le n,$ and we have the following result :
\begin{prop}
If $\rho(T)<0$, the tropical parametrized hyperplane $\mathcal{H}_{a}^{\sigma}$,
given the configuration $\sigma=\{I^{k}\}_{1\le k\le n}$, separates $\mathcal{V}^{k}$
and $\mathcal{V}^{l}$ for all $1 \le k\ne l \le n$ with a margin of at least $-\rho(T)$.
Moreover, this margin is exact in the case where all $T^{k}$ are
of the form
\[
T^{k}(x)=P_{\mathcal{V}^{k}}(x)=\sup_{v\in \mathcal{V}^{k}}\left(v_{i}+\min(-v+x)\right),
\]
 which is in particular the case when separating finite point clouds. 
\end{prop}

\begin{proof}
We consider class $k\in\{1,\ldots, n\}$ and sector $i\in I^\ell$, $\ell \ne k$. Using the same reasoning
as in the proof of Proposition \ref{TODO:CITE}, we obtain that for all $v^{k}\in \mathcal{V}^{k}$,
\[
d_H(\mathcal{S}^\ell,v^{k})=\max(v^{k}-a)-\max_{j\in I^{\ell}}(v^{k}-a)_j\ge-\rho(T).
\]
We now prove the margin exactness when all $T^{k}$ are of the aforementioned form. 

Let $i\in\{1,\ldots, d\}$. As $T(a)=\rho(T)+a$, there are two distinct classes $k$ and $l$ such that  $\inf(T^{k}(a)_{i}, T^{l}(a)_{i})=\rho(T)+a_{i}$. By symmetry, suppose that $T^k(a)_i=\rho(T)+a_i$, meaning that $i\in I_\ell \subset \bigsqcup_{p\ne k} I_p$. Using the form of $T^k(a)_i$ and adapting the ideas of Proposition \ref{TODO:CITE}, for $\varepsilon > 0$, we can find $v\in \mathcal{V}^k$ such that $$\max(v^{k}-a)-(v_{i}^{k}-a_{i})\le-\rho(T)+\varepsilon,$$ which means that 
\[
d_H(\mathcal{S}^\ell,\mathcal{V}^{k})\le \max(v^{k}-a)-\max_{j\in I_\ell}(v^{k}-a)_j \le \max(v^{k}-a) - (v^{k}_i - a_i) \le -\rho(T)+\varepsilon.
\]

Hence, each sector is at distance at most $-\rho(T)$ from one point of another class: the margin is exact.
\end{proof}

\section{Tropical Polynomials as Piecewise Linear Classifiers}

In this section, we extend our previous approach to piecewise linear
fitting, using tropical polynomials. 

A \emph{tropical polynomial} \emph{function} over $\mathbb{R}_{\max}^{d}$
is defined by
\[
f(x)=\bigoplus_{\alpha\in\mathbb{Z}^{d}}f_{\alpha}x^{\alpha}=\max_{\alpha\in\mathbb{Z}^{d}}\left(f_{\alpha}+\langle x,\alpha\rangle\right),
\]
where $f_{\alpha}=-\infty$ except for finitely many values of $\alpha$,
and with $\langle x,\alpha\rangle=\sum_{i}x_{i}\cdot\alpha_{i}$ under
the convention $(-\infty)\cdot0=0$. The \emph{tropical hypersurface} associated with $f$ is the set of points from
$\mathbb{R}_{\max}^{d}$ where the maximum in $f$ is attained twice.
Tropical hypersurfaces are piecewise linear and divide $\mathbb{R}_{\max}^{d}$
between sectors corresponding to the maximal monomial: they can perform
more complex classification.

If $\mathcal{A}\subset\mathbb{Z}^{d}$ is a set of vectors, we define
the \emph{Veronese embedding} of $x$ as:
\[
\text{ver}_{\mathcal{A}}(x):=\left(\langle x,\alpha\rangle\right)_{\alpha\in\mathcal{A}}\in\mathbb{R}_{\max}^{\mathcal{A}},
\]
allowing us to map our point space into a larger space, made up of
various integer combinations of features. To take a fairly exhaustive
subset of monomials, we consider the integer combinations defined
by the integer points of the dilated simplex. Noting $s\in\mathbb{N}^{*}$
a scale parameter, we define
\[
\mathcal{A}_{s}:=\left\{x\in\mathbb{N}^{d},\quad x_{1}+\cdots+x_{d}=s\right\},
\]
which grows polynomially with $s$. The Veronese embedding defined
by $\mathcal{A}_{s}$ is invertible and if $\mathcal{H}_a$ is a tropical
hyperplane of apex $a$ in $\mathbb{R}_{\max}^{\mathcal{A}}$, $\text{ver}_{\mathcal{A}_{s}}^{-1}(\mathcal{H})$
is a tropical polynomial hypersurface in the initial space, corresponding to the polynomial
\[ f_{-a, \mathcal{A_s}}(x)=\bigoplus_{\alpha\in \mathcal{A}_s} (-a_\alpha)x^\alpha \]
Hence, $\text{ver}_{\mathcal{A}_{s}}$ is a feature map
enabling us to perform piecewise linear classification in $\mathbb{R}_{\max}^{d}$. This brings us close to the framework of neural networks with piecewise linear activations, e.g. relaxed linear units \cite{zhang2018tropical}. $\mathcal{A}_1$ corresponds to tropical hyperplanes: we are generalizing the approach developed in Section \ref{sec2}.

The initial metric can be easily linked to the metric in the augmented space: for $x\in\mathbb{R}_{\max}^d$, we have
\begin{align*}
\lVert\text{ver}(x)\rVert_{H,\mathbb{R}^{\mathcal{A}_s}_{\max}}&=\max_{\alpha\in\mathcal{A}_{s}}\sum_{i}\alpha_{i}x_{i}-\min_{\alpha\in\mathcal{A}^{s}}\sum_{i}\alpha_{i}x_{i}\\&\le s(\max x-\min x)=s\lVert x\rVert_{H,\mathbb{R}_{\max}^d}.
\end{align*}

We deduce the following result:
\begin{prop}
If data is separable in the augmented space, noting $(a, \rho(T))$ the eigenpair of $T$ applied to $\mathbb{R}_{\max}^s$, data is separable in the initial space by the tropical hypersurface of $f_{-a,\mathcal{A}^s}$ with a margin of at least $-\rho(T)/s$.
\end{prop}

As the augmented data is scaled by a factor of $s$, $\rho(T)$ should often grow linearly with $s$, meaning we have a good margin guarantee at the end.

In Figures \ref{fig:iris}, \ref{fig:circular} and \ref{fig:toy_reverse}, we visualize the decision boundaries of the piecewise linear classifier. Dotted lines in light gray indicate merged sectors after assignment, and margin lower estimation is represented by the Hilbert balls around each point.

These methods obviously work in higher dimensions, but we can only visualize them in lower ones. Note that in the multi-class case, where classes are not separable in pairs, the result is consistent, although we have not provided a theoretical guarantee in this case.

\subsection*{Complexity}

The learning speed depends essentially on the dimension of the feature space, i.e. the starting dimension and the number of monomials considered. Although our iteration scheme is efficient, given that $$|\mathcal{A}^s| = \binom{s+d-1}{s},$$ the complexity grows very quickly, opening the way to feature selection issues for directly learning \emph{sparse} polynomials.

On the other hand, when the tropical polynomial is trained, since only active monomials are kept, inference is $\mathcal{O}(rd)$, where $r$ is the number of monomials considered, which is very fast.

TODO : ON LOG PAPER (quelques mots)

\bigskip
\bigskip
\bibliographystyle{plain}
\bibliography{ea}

\end{document}
