\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[preprint]{neurips_2024}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{pgfplots}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{xcolor}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{adjustbox}
\usepackage{graphicx}
\usepackage{titlesec}
\usepackage{subcaption}
\usepackage[colorinlistoftodos,bordercolor=orange,backgroundcolor=orange!20,linecolor=orange,textsize=scriptsize]{todonotes}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{proposition}[theorem]{Proposition}

\newcommand{\R}{\mathbb{R}}
\newcommand{\Rmax}{\mathbb{R}_{\max}}
\newcommand{\trop}{\mathbb{T}}
\newcommand{\T}{\mathbb{T}}

\title{Efficient Tropical SVMs via Mean-Payoff Games}

\author{
  Xavier Allamigeon \\
  Inria and CMAP \\
  École polytechnique \\
  Palaiseau, France \\
  \And
  Samuel Boïté \\
  Inria and CMAP \\
  École polytechnique \\
  Palaiseau, France \\
  \And
  Stéphane Gaubert \\
  Inria and CMAP \\
  École polytechnique \\
  Palaiseau, France \\
  \And
  Théo Molfessis \\
  Inria and CMAP \\
  École polytechnique \\
  Palaiseau, France \\
}

\begin{document}

\maketitle
\todo[inline]{SG :title is very good. I hesitated with Scalable Tropical SVMs via Mean-Payoff Games? but perhaps effficient is stronger.}
\begin{abstract}
   In 2006, Gärtner and Jaggi introduced a tropical analogue of support vector machines, using a single tropical hyperplane in dimension $n$ to separate $n$ classes of points. Efficiently computing tropical separators has remained an open problem.
   We introduce an algorithm for Tropical Support Vector Machines that overcomes the combinatorial explosion of previous approaches.
   %Tropical hyperplanes partition the input space into multiple sectors, naturally supporting multi-class classification.
Our main result shows that the spectral radius of a specially constructed Shapley operator fully characterizes separability and margin: data are tropically separable if and only if the spectral radius is negative, with an optimal margin, both in the separable and inseparable case, is determined by this spectral
   raidus. This provides a reduction to mean-payoff games, a well studied class of problems in algorithmic game theory.
   %% case  equal to the negative of this radius.
   %% In the non-separable setting, the spectral radius characterizes the overlap between classes. 
   This approach enables computing of an optimal separating hyperplane via scalable iterative algorithms -- with a complexity linear in the size of the data set and pseudo-polynomial in the desired precision.\todo{SG: added linearity (otherwise pseudo may look a bit weak)}
   Finally, we combine tropical classifiers with linear feature maps to construct piecewise-linear classifiers.
   %% We extend this framework to tropical polynomials, enabling more flexible decision boundaries while maintaining margin guarantees.
\end{abstract}

\section{Introduction}\label{sec:intro}

Classification is a fundamental task in machine learning, and Support Vector Machines (SVMs) have been a cornerstone method for decades. Traditional SVMs create decision boundaries using affine hyperplanes, which provide maximum-margin separation with strong generalization guarantees \cite{vapnik1999}. However, these linear boundaries become limiting when faced with complex, nonlinear data patterns, typically requiring kernel methods or feature engineering \cite{scholkopf2002}.

\paragraph{Motivation: Beyond Linear Boundaries.} We build on\todo{SG: explore $\to$ build on?} tropical algebra, a framework where standard addition becomes the maximum operation, and multiplication becomes addition \cite{maclagan2015}. This leads to different geometric structures with attractive properties for machine learning: (1) instead of creating binary partitions, tropical hyperplanes divide space into multiple sectors, making them naturally suited for multi-class problems; (2) their piecewise-linear nature captures more complex patterns while maintaining interpretability; (3) the resulting decision boundaries coincide with those created by modern deep learning models with ReLU activations~\cite{zhang2018}.\todo{SG: reinforced ``coincide''} These properties provide richer, yet interpretable, decision boundaries that can capture nonlinear patterns in data while maintaining computational tractability \cite{maragos2021}.

Tropical geometry has emerged as a powerful tool for modeling piecewise-linear phenomena in machine learning. Together with polyhedral geometry,
it has been used to bound the number of linearity regions ot functions realized by these networks~\cite{zhang2018,montufar}.\todo{SG: added this}
It has been successfully applied to linear regression \cite{maragos2020,akian2020},\todo{SG added \cite{maragos2020}} principal component analysis \cite{yoshida2019}, neural network analysis \cite{maragos2021}, and clustering \cite{monod2022}. Related work by Fotopoulos et al.~\cite{fotopoulos2024}\todo{quote all or none} has explored neural network compression through tropical geometry, demonstrating the broader utility of these techniques.

\paragraph{Previous Work on Tropical SVMs.} Gärtner and Jaggi \cite{gartner2008} introduced tropical SVMs using linear programming formulations. Their work introduced an elegant geometric approach to multiclass problems, since a single tropical hyperplane in dimension $d$ partitions the space in $d$ regions.
%% partiidemonstrated that tropical hyperplanes provide improved locality in decision boundaries, where only nearby support vectors influence classification, unlike classical SVMs where all support vectors contribute globally.\todo{SG: I am not sure it is a quality (less stable than L1), perhaps just mention that this is an elegant geometric approach to multiclass problem.}
Despite these theoretical benefits, their method required exploring all possible sector assignment combinations, leadi
ng to exponential worst-case complexity. This computational barrier has limited practical applications.
Tang et al.~\cite{tang2020} and Monot et al~\cite{monod2022} later developed specialized algorithms for binary classification cases where data points from the same category stay in the same sector, showing promising results in computational biology for analyzing evolutionary trees.\todo{quote lavishly yoshida}

\paragraph{From Game Theory to Machine Learning.} Our approach uses concepts from game theory—specifically, Shapley operators from zero-sum dynamic games. Shapley introduced and ``operator appraoch'' to solve discounted games~\cite{shapley1953}, the operators we use here arise in the study of games in infinite horizon in which the objective function is a mean-payoffper time unit \cite{zwick1996}.\todo{SG: quote gillette version mean payoff}
%~\cite{kolokoltsov1997}.
The spectral properties of these operators have been extensively studied for analyzing fixed points and convergence in nonlinear systems~\cite{kolokoltsov1997,gaubert2004}. In particular, the spectral radius coincides with the value of the associated mean-payoff game. Mean-payoff games belong to the complexity class NP $\cap$ coNP and their
membership to P is a long standing open problem. However, huge instances of mean-payoff games
can be solved by fast iterative methods, like relative Krasnoselskii-Mann
iteration, whose complexity is bounded by $O(L/\epsilon^2)$, {\em linearly} in the input size $L$,
and pseudopolynomially in the requested precision $\epsilon$. 
%which have a linear complexity in the input size, Whereas the existence of a polynomial-time algort
%, a class of two-player zero-sum games, are closely linked to the spectral radius of such operators \cite{zwick1996}.
\todo[inline]{SG Delete (we need a broader state of the art on mean-payoff games -- and the theorem there only concerns approximating $\rho$ in $1/\epsilon$, here we need to approximate the eigenvector which is in $1/\epsilon^2$): Importantly, recent advances in solving these games efficiently \cite{allamigeon2025} provide efficient algorithms that we can apply to tropical classification. -- Indeed, we need instead to use the relative KM algorithm along the lines of~\cite{akiangaubertqisaadi}, with a complexity in $1/\epsilon^2$, but this algorithm provides the eigenvector. \cite{allamigeon2025} does not (only the eigenvalue is approximated, we can get sub and super eigenvectors however with a preoprocessing). The termination condition should be changed in the algo below to $\|T(x)-x\|_H\leq \epsilon$.}

\paragraph{Contributions.} We develop a new approach to tropical classification using mean-payoff games,
overcoming the computational limitations of previous approaches:

\begin{enumerate}
    \item We establish a direct connection between tropical separability and the spectral radius $\rho(T)$ of a Shapley operator constructed from class-specific tropical projections.
    
    \item We prove that when data are separable, the optimal margin equals $-\rho(T)$. Moreover, in the binary non-separable case, $\rho(T)$ quantifies exactly how much the data points would need to be perturbed to achieve separability.
    
    \item We develop an algorithm based on mean-payoff games and Krasnoselskii--Mann iteration that computes the optimal classifier in a time that is linear in the input size. %pseudo-polynomial time rather than exponential time.
    
    \item We extend our framework to tropical polynomial classifiers, enabling more expressive decision boundaries while preserving theoretical margin guarantees.\todo{Say more bout piecewise linear / feature map, useful in application}
\end{enumerate}

This work makes tropical SVMs tractable for real-world applications, enabling natural multi-class classification, and opening new directions for piecewise-linear methods that balance expressivity, interpretability, and computational efficiency.

\begin{figure}[h]
    \centering
    \resizebox{0.99\textwidth}{!}{\clipbox{0.15\width{} 0.30\height{} 0.15\width{} 0.30\height{}}{\input{figures/moons.pgf}}}
    \caption{Visualization of a degree-2 tropical polynomial classifier. Each region corresponds to a sector where a specific affine combination of the features dominate, creating an interpretable piecewise-linear decision boundary. Inference remains computationally efficient, requiring only the evaluation of the dominant monomial at each test point.}
    \label{fig:tropical_hyperplane}
\end{figure}

The remainder of the paper is organized as follows. Section~\ref{sec:prelim} introduces the essential concepts from tropical geometry. Section~\ref{sec:spectral} presents our spectral framework and main theoretical results, showing how tropical separability connects to spectral properties. Section~\ref{sec:algorithm} details our algorithm and implementation, explaining how we achieve pseudo-polynomial complexity. Section~\ref{sec:polynomials} extends the framework to tropical polynomials for more expressive decision boundaries. Section~\ref{sec:maslov} explores connections with classical SVMs, and Section~\ref{sec:discussion} discusses limitations and future directions. The code to reproduce our figures and experiments is available at \url{https://github.com/samuelbx/tropical-svm}.

\section{Tropical Geometry Preliminaries}\label{sec:prelim}

We now introduce the key concepts from tropical geometry that form the foundation of our approach. To make these abstract concepts more accessible, we include intuitive explanations alongside formal definitions.

\paragraph{The Max-Plus Semiring.}
The tropical (or max-plus) semiring $\trop = \R \cup \{-\infty\}$ replaces traditional arithmetic operations with:
\begin{align}
x \oplus y &= \max(x,y) \quad \text{(tropical addition)} \\
x \odot y &= x + y \quad \text{(tropical multiplication)}
\end{align}

These operations may seem strange at first, but they naturally model systems where we care about ``bottlenecks'' or ``critical paths.'' For example, in project planning, if task A takes $x$ days and task B takes $y$ days, the project completion time depends on the maximum ($x \oplus y$) of these durations if tasks are parallel, and their sum ($x \odot y$) if sequential.

\paragraph{Tropical Projective Space.}  
The tropical projective space identifies points that differ by adding the same constant to all coordinates. Formally, it's the quotient of $\trop^d \setminus \{(-\infty,\dots,-\infty)\}$ by the equivalence relation $x \sim y$ if $x = y + c \cdot \mathbf{1}$ for some constant $c$. In practice, we embed data from $\R^d$ into the projective space via:
\[
x=(x_1,\dots,x_d)\mapsto (x_1,\dots,x_d,-(x_1+\cdots+x_d))
\]

This transformation makes our classifier invariant to shifts—adding the same constant to all features doesn't change the classification. It's similar to how projective geometry in computer vision makes analysis invariant to camera distance.

\paragraph{Tropical Hyperplanes and Sectors.}
A tropical hyperplane with apex $a \in \trop^d$ is defined as:
\begin{align}
\mathcal{H}_a = \left\{ x \in \trop^d : \text{the maximum of }(x_i + a_i)\text{ over $1\leq i\leq d$ is attained at least twice} \right\}
\end{align}

This hyperplane divides the space into at most $d$ sectors. The $i$-th sector contains points where the maximum of $x + a$ occurs at the $i$-th coordinate:\todo{include a picture of tropical hyperplane in the body}
\begin{align}
S_i(a) = \{x \in \trop^d : i \in \underset{j}{\arg\max} (x_j + a_j)\}
\end{align}

Unlike classical hyperplanes that create two half-spaces, tropical hyperplanes create multiple sectors—one for each dimension. They naturally support multi-class classification, where we can assign different sectors to different classes.

\paragraph{Hilbert Seminorm and Tropical Distance.}
The Hilbert seminorm\todo{SG: I would delete ``in tropical geometry''} measures the ``spread'' of coordinates:
\begin{align}
\|x\|_H = \max_i x_i - \min_i x_i
\end{align}

This induces a projective distance $d_H(x,y) = \|x - y\|_H$ that remains invariant to adding the same constant to all coordinates \cite{cohen2004}. We use this distance to define margins in tropical classification. For classification, it tells us how confidently a point belongs to its assigned sector rather than another sector.

\paragraph{Tropical Convexity and Projections.}
A set $C \subset \trop^d$ is a \emph{tropical convex cone} if for all $x,y$ in $C$ and coefficients $\lambda,\mu$ in $\trop$,
%with $\lambda \oplus \mu = 0$ (meaning $\max(\lambda,\mu)=0$),
the point $(\lambda \odot x) \oplus (\mu \odot y)$ is also in $C$ \cite{cohen2004,develin2004}.
The tropical convex hull of points $\{x_1,\ldots,x_p\}$ is defined as:
\begin{align}
  \text{cone}_{\max}(X) = \left\{\bigoplus_{i=1}^p \lambda_i \odot x_i : \lambda_i \in \trop\} \enspace .
%  ,\, \bigoplus_{i=1}^p \lambda_i = 0 \right\}
\end{align}

The tropical projection $P_X(y)$ of a point $y$ onto this convex hull is:
\begin{align}
P_X(y) = \max\{z \in \text{cone}_{\max}(X) : z \leq y\}
\end{align}
\todo[inline,color=red!30]{SG. This is the projection on a cone, not on a convex set, perhaps we need to speak only of cones (modulo passing to the projective space), then this is ok. For a convex set, the max may be taken over a nonempty set}
Tropical convexity generalizes the idea of conventional convexity to the max-plus setting. A conical tropical convex hull contains all tropical linear combinations of vectors. Moreover, the projection finds a ``closest'' point in the conical convex hull~\cite{AGNS10}.% that doesn't exceed our target point in any coordinate.
\todo{SG: cite also Chepoi}

These tropical projections will play a central role in our classification framework. We will use them to build class-specific operators that characterize the separability of data.

\section{Spectral Framework for Tropical SVMs}\label{sec:spectral}

Having established the basics of tropical geometry, we now develop our spectral approach to tropical classification. The key insight is connecting the separability of data classes to the spectral properties of a specially constructed operator.

\paragraph{Shapley Operators and Their Spectral Theory.}
A Shapley operator $T: \R^d \to \R^d$ satisfies two fundamental properties \cite{kolokoltsov1992}:
\begin{enumerate}
    \item \textit{Monotonicity:} If $x \leq y$ coordinatewise, then $T(x) \leq T(y)$ coordinatewise
    \item \textit{Additive homogeneity:} For any constant $\alpha \in \R$, $T(\alpha + x) = \alpha + T(x)$,
      where $\alpha +x$ denotes the vector obtained by adding $\alpha$ to every entry of $x$.
\end{enumerate}
It admits a unique continuous extension $\T^d\to \T^d$, also denoted by $T$.\todo{cite Sparrow (Akian,gaubert,guterman 2012)} Then, the spectral radius of $T$ is defined as:
\begin{align}
\rho(T) = \max\{\lambda \in \R : \exists u \neq -\infty \text{ with } T(u) = \lambda + u\} \enspace,
\end{align}
where $-\infty\in \T^n$ denotes the all $-\infty$ vector -- in other words, the ``tropical zero'' vector.
The maximum is always achieved.

Equivalently, $\rho(T)$ is the smallest value of $\lambda\in \R$ for which there exists a vector $u\in\R^d$
satisfying $T(u) \leq \lambda + u$ \cite{nussbaum1986,AGGut10,akiangaubertqisaadi}.\todo{quote akian...}

\paragraph{Constructing the Classification Operator.}
Consider a classification problem with $K$ classes, each represented by a set of points $X^1,\dots,X^K \subset \trop^d$. We define an operator $T^k$ for each class $k$ by taking the tropical projection onto the convex hull of points in that class:
\[
T^k(x) = P_{X^k}(x)
\]

We then combine these operators into a single classification operator $T$ defined coordinatewise as:
\[
T(x)_i = \operatorname{\max}_2\{T^1(x)_i, \dots, T^K(x)_i\}
\]
where $\operatorname{\max}_2$ denotes the second-largest value among the outputs. For binary classification ($K=2$), this simplifies to $T(x)=\min\{T^1(x), T^2(x)\}$.

\paragraph{Main Theorem: Spectral Separability.}
Our central result establishes the connection between tropical separability and the spectral radius:\todo[inline]{SG: does not some part of the theorem (the last one?) requires to use the diagonal free variant? }
\todo{check whether everything works with DF and that teh satrt with any $P$ such that $x\leq Px$ if x fixed point. Then restate}
\begin{theorem}[Spectral Separability Criterion]\label{thm:spectral_separability}
Let $X^1,\ldots,X^K \subset \trop^d$ be labeled point sets and let $T$ be the classification operator defined above. Then:

(1) \textit{Separability Criterion:} The data are tropically separable by a hyperplane if and only if $\rho(T) < 0$.

(2) \textit{Margin Optimality:} In the separable case, the maximum achievable margin equals $-\rho(T)$.

(3) \textit{Soft-Margin Interpretation:} For binary classification with overlapping data, $\rho(T)$ is positive and quantifies the minimal perturbation needed to achieve separability.

Moreover, both $\rho(T)$ and an associated apex vector $a$ satisfying $T(a) = \rho(T)+a$ can be computed in pseudo-polynomial time using mean-payoff game algorithms (see Section~\ref{sec:algorithm}).
\end{theorem}

\emph{Proof Sketch.}
The proof follows two main directions. First, if there exists a tropical hyperplane $\mathcal{H}_a$ separating the classes with margin $\gamma>0$, then using the monotonicity and homogeneity properties of the operator, we can show $T(a) \leq a - \gamma$. This implies $\rho(T) \leq -\gamma < 0$.

Conversely, if $\rho(T)<0$, we can find an eigenvector $a$ satisfying $T(a)=\rho(T)+a$. Using this apex, we can construct a separating hyperplane by assigning sectors based on which class operator dominates at each coordinate. This hyperplane achieves a margin of exactly $-\rho(T)$.

For the non-separable case, we show that $\rho(T)$ quantifies the minimum perturbation required to make the data separable by constructing an explicit perturbation of magnitude $\rho(T)$.

A complete proof is provided in Appendix~\ref{appendix:proofs}.

\paragraph{Benefits of the Spectral Approach.} 
Our framework advances the tropical SVM foundation established by Gärtner and Jaggi \cite{gartner2008}. Their work showed that tropical SVM could be formulated as finding a point of maximum margin within a tropical polytope defined by sector constraints, but required exhaustive exploration of sector assignments—leading to exponential complexity.

In contrast, our spectral characterization offers a complete theoretical understanding of when data are tropically separable, an exact formula for the optimal margin, an efficient algorithm to find the optimal classifier without combinatorial exploration and a natural interpretation for non-separable cases.

\paragraph{Limitations.} 
The primary limitation of our spectral approach is its sensitivity to outliers. Since tropical convex hulls encompass all points in each class, a single misplaced point can significantly alter the classification boundary and potentially render previously separable data inseparable.

Additionally, because our method is formulated as a spectral criterion rather than an optimization problem, it's challenging to incorporate relaxation mechanisms that would handle misclassified points in a controlled way. Traditional SVMs use slack variables to allow soft margins and tolerate some misclassifications, but our current approach doesn't have a direct equivalent. This suggests an important direction for future research in tropical classification.

\section{Algorithm and Implementation}\label{sec:algorithm}

Having established the theoretical foundation, we now present our algorithm for tropical SVM based on the spectral criterion. The key insight is that we can compute the spectral radius and optimal hyperplane efficiently without exploring all possible sector assignments.

\paragraph{Efficient Computation of Tropical Projections.}
\label{subsec:computing_projections}
The first step is to compute the tropical projections efficiently. The tropical projection $P_X(y)$ of a point $y$ onto the tropical convex hull of a set $X \subset \trop^d$ can be computed using:
\[
P_X(y)_i = \max_{1 \leq j \leq p} \left\{X_{ij} + \min_{1 \leq k \leq d} \left(-X_{kj} + y_k\right)\right\}
\]
where $p$ is the number of points in $X$ and $X_{ij}$ is the $i$-th coordinate of the $j$-th point.

This formulation can be viewed as a mean-payoff game \cite{akian2020} and computed in $\mathcal{O}(pd)$ time—linear in both the number of points and dimensions.

\paragraph{Computing the Spectral Radius with Krasnoselskii-Mann Iterations.}
\label{subsec:spectral_computation}
To compute the spectral radius $\rho(T)$ and a corresponding eigenvector $a$, we apply a Krasnoselskii-Mann iteration scheme \cite{nussbaum1986, gaubert2004}, outlined in Algorithm~\ref{alg:km_iteration}.\todo{SG: very good to quote \cite{nussbaum1986,gaubert2004} but KM is not there, we should rather quote the original works of Baillon and Bruck~\cite{baillonbruck} and perhaps followup by cominetti et al, and see~\cite{akiangaubertqisaadi,akianmfcs} for the relative KM which we use here.}

\begin{algorithm}
\caption{Krasnoselskii--Mann Iteration for Tropical SVM}\label{alg:km_iteration}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Shapley operator $T$, convergence threshold $\varepsilon > 0$
\STATE \textbf{Initialize:} $x^{(0)} \in \R^d$ (typically set to $\mathbf{1}_d$), $\lambda^{(0)} = 0$
\FOR{$k = 0, 1, 2, \ldots$ until convergence}
  \STATE $z^{(k+1)} \leftarrow \frac{1}{2}\bigl(x^{(k)} + T(x^{(k)})\bigr)$ \COMMENT{Average the current point with its image to stabilize}
  \STATE $x^{(k+1)} \leftarrow z^{(k+1)} - \max\bigl(z^{(k+1)}\bigr)\,\mathbf{1}_d$ \COMMENT{Project onto the tropical projective space}
  \STATE $\lambda^{(k+1)} \leftarrow 2\max\bigl(z^{(k+1)}\bigr) - \max\bigl(x^{(k)}\bigr)$ \COMMENT{Estimate the current spectral radius}
  \IF{$|\lambda^{(k+1)} - \lambda^{(k)}| < \varepsilon$ \COMMENT{Check if the spectral radius has stabilized}}
    \STATE \textbf{break}
  \ENDIF
\ENDFOR
\STATE \textbf{Return:} $\rho(T) \approx \lambda^{(\text{final})}$, $a \approx x^{(\text{final})} - \frac{1}{d}\sum_{i=1}^d x_i^{(\text{final})} \cdot \mathbf{1}_d$
\end{algorithmic}
\end{algorithm}
\todo[inline]{SG: to be updated}
Recent results by Allamigeon et al. \cite{allamigeon2025} show that this iteration achieves an $\varepsilon$-approximation in $O(1/\varepsilon)$ iterations. By avoiding the combinatorial exploration of sector assignments, our method runs in pseudo-polynomial time—specifically $O\bigl(\frac{nd}{\varepsilon}\bigr)$ for $n$ data points in $d$ dimensions.

\paragraph{The Complete Tropical SVM Algorithm.}\label{subsec:complete_algorithm}
Algorithm~\ref{alg:tropical_svm} presents our complete procedure for tropical SVM classification.

\begin{algorithm}
\caption{Tropical SVM}\label{alg:tropical_svm}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Labeled point sets $X^1,\dots,X^K \subset \trop^d$
\STATE \textbf{Construct Operators:}
  \STATE \quad For each class $k$, compute the tropical projection operator $T^k(x) = P_{X^k}(x)$
  \STATE \quad Combine via $T(x)_i = \operatorname{\max}_2\{T^1(x)_i, \dots, T^K(x)_i\}$ for each coordinate $i$
\STATE \textbf{Compute Spectral Properties:}
  \STATE \quad Run Algorithm~\ref{alg:km_iteration} to obtain $\rho(T)$ and apex $a$
\STATE \textbf{Assign Sectors to Classes:}
  \IF{$\rho(T) < 0$ \COMMENT{Separable case}}
    \STATE Assign coordinate $i$ to class $k$ if $T^k(a)_i > \rho(T) + a_i$ \COMMENT{Assignment based on dominant operator}
  \ELSE
    \STATE Use a heuristic (e.g., majority vote) for non-separable data \COMMENT{Fall back to best-effort assignment}
  \ENDIF
\STATE \textbf{Output:} Apex $a$, margin $-\rho(T)$ (if separable), and sector assignments for classification
\end{algorithmic}
\end{algorithm}

\paragraph{Empirical Performance Analysis.}
We validated the computational complexity of our algorithm through empirical testing (detailed in Appendix~\ref{appendix:empirical}). Our implementation achieves tractable performance on standard benchmark datasets, with performance scaling linearly with both dataset size and feature dimension as predicted by our theoretical analysis.

For the first time, we demonstrate that tropical SVMs can be practically computed with pseudo-polynomial guarantees, removing the exponential barrier that limited previous approaches. While specialized classical SVM libraries remain faster on conventional tasks, our approach enables new applications where piecewise-linear decision boundaries offer advantages.

Future optimizations could include kernel methods, sparse representations for large datasets, and parallel implementations to further improve performance on high-dimensional problems.

\section{Tropical Polynomials for Enhanced Expressivity}\label{sec:polynomials}

While tropical hyperplanes provide effective classification boundaries, we can achieve even greater expressivity by extending our framework to tropical polynomials. These polynomials create more flexible decision boundaries while maintaining theoretical guarantees.

\paragraph{Tropical Polynomial Kernel.}
A tropical polynomial in $\trop^d$ takes the form:
\begin{align}
f(x) = \max_{\alpha \in A} (c_\alpha + \langle \alpha, x \rangle)
\end{align}
where $A \subset \mathbb{Z}^d$ is a finite set of integer vectors (monomials), $c_\alpha \in \R$ are coefficients, and $\langle \alpha, x \rangle = \alpha_1 x_1 + \cdots + \alpha_d x_d$ is the tropical dot product.

To fit such polynomials, we use a feature map $\Phi_A: \trop^d \to \trop^{|A|}$ defined by:
\begin{align}
[\Phi_A(x)]_\alpha = \langle \alpha, x \rangle
\end{align}

This feature map transforms the original data into a higher-dimensional space where each coordinate corresponds to a monomial term. A tropical hyperplane in this feature space corresponds to a tropical polynomial in the original space. This is conceptually similar to the kernel trick in classical SVMs, but with an explicit feature mapping based on tropical algebra.

\paragraph{Strategic Monomial Selection.}
The choice of monomial set $A$ critically affects both the expressivity and computational complexity of the resulting classifier. We explore two complementary approaches:

\begin{enumerate}
    \item \textbf{Homogeneous Monomials:} We use all monomials of degree $s$, defined as 
    $A_s = \{\alpha \in \mathbb{N}^d : \sum_i \alpha_i = s\}$. The number of such monomials is $\binom{s+d-1}{s}$, which grows polynomially with $s$ for fixed dimension $d$.
    
    \item \textbf{Adaptive Selection:} We sample pairs of points from different classes and construct monomials corresponding to the slopes of separating lines between them. This approach focuses computational resources on the most discriminative monomials.
\end{enumerate}

The degree parameter $s$ controls the trade-off between expressivity and overfitting. Higher degrees create more flexible boundaries but may overfit the training data. Cross-validation can guide this selection process.

Importantly, only a sparse subset of monomials typically becomes active in the final classifier, making the approach computationally efficient even with many candidate monomials.

\paragraph{Classification with Tropical Polynomials.}\label{subsec:poly_classification}
To perform classification with tropical polynomials:

\begin{enumerate}
    \item Map the original data to the feature space: $\Phi_A(X^k) = \{\Phi_A(x) : x \in X^k\}$ for each class $k$.
    
    \item Apply the tropical SVM algorithm in this feature space to find apex $a \in \trop^{|A|}$ and spectral radius $\rho(T)$.
    
    \item The classifier in the original space is the tropical polynomial:
    \begin{align}
    f_a(x) = \max_{\alpha \in A} (-a_\alpha + \langle \alpha, x \rangle)
    \end{align}
    
    \item Classify new points by identifying which sector of $f_a(x)$ they fall into.
\end{enumerate}

\paragraph{Margin Guarantees for Tropical Polynomials.}
When extending to polynomial classifiers, we maintain theoretical guarantees on the margin. Specifically, when $\rho(T)<0$ (indicating separability in the feature space), the data are separable in the original space with a margin of at least $-\rho(T)/\lVert \Phi_A\rVert_{\text{op},\infty}$, where $\lVert \Phi_A\rVert_{\text{op},\infty}$ is the operator norm of the feature map.

For homogeneous degree-$s$ monomials, this simplifies to $-\rho(T)/s$, meaning the margin scales inversely with the polynomial degree. This gives a principled way to balance expressivity with generalization through the choice of polynomial degree.

This approach maintains the core theoretical guarantees of the hyperplane formulation while substantially increasing model flexibility, as illustrated in Figures~\ref{fig:homogeneous_selection} and \ref{fig:adaptive_polynomial} on the following pages.

\newpage

\vspace*{2em}
\begin{figure}[ht!]
    \centering
    \resizebox{0.9\textwidth}{!}{\clipbox{0.15\width{} 0.30\height{} 0.15\width{} 0.30\height{}}{\input{figures/blobs.pgf}}}
    \caption{Multi-class classification using a cubic tropical polynomial classifier (degree-3 monomials). Each color represents a different class, and the boundaries show where the dominant monomial changes. Note how the polynomial naturally separates the five clusters with piecewise-linear boundaries. The light orange band around the decision boundaries indicates the margin, which maintains a guaranteed lower bound of $-\rho(T)/3$ due to our theoretical results.}
    \label{fig:homogeneous_selection}
\end{figure}
\vspace*{4em}
\begin{figure}[ht!]
    \centering
    \resizebox{0.9\textwidth}{!}{\clipbox{0.15\width{} 0.30\height{} 0.15\width{} 0.30\height{}}{\input{figures/moons-feat-sel.pgf}}}
    \caption{Visualization of a tropical polynomial classifier using the adaptive monomial selection strategy described in Section~\ref{sec:polynomials}. Rather than using all possible monomials, this approach selects terms based on pairs of points from different classes, focusing computational resources on the most discriminative features. The resulting boundary adapts closely to the data's structure while maintaining margin guarantees.}
    \label{fig:adaptive_polynomial}
\end{figure}
\vspace*{2em}


\newpage
\section{Connection to Classical SVMs}\label{sec:maslov}
Tropical hyperplanes can be seen as limits of classical hyperplanes under logarithmic scaling, a process known as Maslov dequantization \cite{viro2001}. This connection suggests a naive approach to tropical SVM: raise the data to a power $\beta > 0$, apply a classical SVM in the transformed space to maximize the margin, then map the result back via a logarithm. As $\beta$ tends to infinity, the resulting decision boundary converges to a tropical hyperplane.

However, this method suffers from severe numerical instability for large $\beta$ and fails to guarantee a good margin in the original space. The limiting hyperplane is typically suboptimal compared to the true tropical classifier obtained through our spectral approach, as illustrated in Figure~\ref{fig:maslov_dequantization}.

\begin{figure}[h]
    \centering
    \resizebox{0.5\textwidth}{!}{\clipbox{0.15\width{} 0.15\height{} 0.15\width{} 0.15\height{}}{\input{figures/log-log.pgf}}}
    \caption{Comparison of tropical hyperplanes obtained through Maslov dequantization and our spectral approach. The limiting hyperplane (red) from dequantization is suboptimal compared to the spectral tropical classifier (black).}
    \label{fig:maslov_dequantization}
\end{figure}

Interestingly, the connection could be exploited in the reverse direction: tropical SVMs may offer efficient, interpretable warm-starts for classical methods, especially in high-dimensional or large-scale settings.

\section{Discussion and Future work}\label{sec:discussion}

We addressed a key computational barrier limiting tropical SVM's practical application in machine learning. By reformulating tropical classification through spectral theory and mean-payoff games, we reduced complexity from exponential to pseudo-polynomial, making tropical SVMs feasible for real-world applications. Our framework provides natural multi-class capabilities, theoretical margin guarantees, and interpretable piecewise-linear decision boundaries.

Our work opens several promising avenues for further investigation. The relationship between tropical polynomials and ReLU networks deserves deeper exploration, potentially yielding insights into network expressivity and inspiring new tropical classifiers. Combining spectral tropical SVMs with classical kernel methods could create powerful hybrid models balancing interpretability with performance. Extending the analysis to derive proper generalization bounds would strengthen the theoretical foundation, particularly understanding how polynomial degree influences sample complexity. Developing tropical analogues to slack variables would address sensitivity to outliers and noisy data. Finally, implementing sparse representations, efficient data structures, and parallel algorithms could extend the approach to large datasets and high-dimensional problems.

\subsection*{Acknowledgements}

TBD

\bibliographystyle{plain}
\bibliography{references}

\newpage
\appendix
\section{Proof of Theorem~\ref{thm:spectral_separability} (Spectral Separability)}\label{appendix:proofs}
We provide a complete proof of our main result on spectral separability.

\begin{lemma}\label{lemma:hyperplane_to_operator}
If the hyperplane $\mathcal{H}_a$ separates point clouds $X^1,\ldots,X^K$ with a margin of at least $\gamma > 0$, then $T(a) \leq -\gamma + a$.
\end{lemma}

\begin{proof}
Consider two different classes $k$ and $\ell$. Let $I^k$ denote the set of coordinates assigned to class $k$. For any point $x \in X^k$, the margin condition implies
\begin{align}
d_H(x, S^{\ell}) = \max_i(x_i - a_i) - \max_{j \in I^{\ell}}(x_j - a_j) \geq \gamma.
\end{align}

This can be rewritten as: for all $i \in I^{\ell}$,
\begin{align}
x_i - a_i \leq \max_j(x_j - a_j) - \gamma.
\end{align}

Taking the maximum over all $x \in X^k$, we get
\begin{align}
\max_{x \in X^k}(x_i - a_i) \leq \max_{x \in X^k}(\max_j(x_j - a_j)) - \gamma.
\end{align}

By the definition of $T^k$, this implies
\begin{align}
T^k(a)_i \leq -\gamma + a_i.
\end{align}

Since this holds for all $i \in I^{\ell}$ and all classes $k \neq \ell$, we have $T(a)_i \leq -\gamma + a_i$ for all $i$, which gives us $T(a) \leq -\gamma + a$.
\end{proof}

\begin{lemma}\label{lemma:operator_to_hyperplane}
If $\rho(T) < 0$, then there exists a hyperplane $\mathcal{H}_a$ that separates the data with a margin of at least $-\rho(T)$.
\end{lemma}

\begin{proof}
Since $\rho(T) < 0$, there exists $a \in \R^d$ such that $T(a) = \rho(T) + a$. We define the sectors as
\begin{align}
I^k = \{i : T^k(a)_i > \rho(T) + a_i\}.
\end{align}

First, we show these sectors are disjoint. If $i \in I^k \cap I^{\ell}$ for $k \neq \ell$, then $T^k(a)_i > \rho(T) + a_i$ and $T^{\ell}(a)_i > \rho(T) + a_i$. But then the second-largest value among $\{T^1(a)_i,\ldots,T^K(a)_i\}$ would exceed $\rho(T) + a_i$, contradicting $T(a)_i = \rho(T) + a_i$.

Next, we show that each point belongs to its assigned sector. For $x \in X^k$ and $i \not\in I^k$, we have
\begin{align}
x_i \leq T^k(x)_i = (T^k(x) - T^k(a))_i + T^k(a)_i.
\end{align}

Since $T^k$ is non-expansive, $(T^k(x) - T^k(a))_i \leq \max_j(x_j - a_j)$. And since $i \not\in I^k$, we have $T^k(a)_i \leq \rho(T) + a_i$. Combining these,
\begin{align}
x_i - a_i \leq \max_j(x_j - a_j) + \rho(T).
\end{align}

With $\rho(T) < 0$, this implies $x_i - a_i < \max_j(x_j - a_j)$, meaning $i$ cannot be the $\arg\max$ of $x - a$. Therefore, the $\arg\max$ must lie in $I^k$, placing $x$ in its correct sector.

Finally, for the margin, consider $x \in X^k$ and sector $S^{\ell}$ with $\ell \neq k$. The distance is.
\begin{align}
d_H(x, S^{\ell}) = \max_j(x_j - a_j) - \max_{i \in I^{\ell}}(x_i - a_i).
\end{align}

Using the inequalities derived above, we get $d_H(x, S^{\ell}) \geq -\rho(T)$.
\end{proof}

For any Shapley operator $T$, we define its fixed-point set as $S(T) = \{x \in \trop^d : x \leq T(x)\}$. An important property is that any tropically convex set $V$ can be written as $V = S(P_V)$. 

In the binary case where data overlaps, we have $T = \min(T^1, T^2)$, hence $S(T)$ characterizes the intersection of convex hulls of both point clouds. As shown in \cite{akian2020}, this implies that $\rho(T)$ equals the inner radius of this intersection, measuring the extent of overlap. In the non-separable case, the apex $a$ given by our Algorithm lies exactly at the center of the inner ball of $V1+ \cap V^2$.

In the multi-class case, the corresponding Shapley operator $T$ can be equivalently expressed as
\begin{align}
T = \max_{1 \leq k < l \leq n}\min(T^k, T^l).
\end{align}
Intuitively, this can be thought of as representing the union of pairwise intersections between data classes, although the $\max$ operator does not strictly correspond to a union of fixed-point sets. This formulation provides an intuition on why our operator effectively characterizes separability across multiple classes.

\begin{figure}[htbp]
    \centering
    \resizebox{0.5\textwidth}{!}{\clipbox{0.15\width{} 0.15\height{} 0.15\width{} 0.15\height{}}{\input{figures/non-separable.pgf}}}
    \caption{Non-separable case: $\rho(T) \geq 0$, indicating class overlap. The spectral radius quantifies the minimum perturbation required to achieve separability, and the inner radius of the convex hulls' intersection.}
    \label{fig:non_separable}
\end{figure}

Before moving to the next proof, we define diagonal-free ``projections'' \cite{gaubert2011}:
\begin{align}
P_X^{\text{DF}}(y)_i = \max_{1 \leq j \leq p} \left\{X_{ij} + \min_{k \neq i} (-X_{kj} + y_k)\right\}.
\end{align}
Both $P$ and $P^\text{DF}$ have identical fixed-point sets and can serve as the class operators $T^k$ in our framework.


\begin{lemma}\label{lemma:perturbation}
For binary classification with $\rho(T) \geq 0$, there exists a perturbation of the point sets $X^1$ and $X^2$, with each point moved by at most $\rho(T)$ in the tropical metric, such that the tropical convex hulls of the perturbed sets have empty intersection.
\end{lemma}

\begin{proof}
Let $a$ be the eigenvector corresponding to $\rho(T)$, i.e., $T(a) = \rho(T) + a$. Let $\mathcal{H}_a$ be the tropical hyperplane with apex $a$. We construct a perturbation by projecting onto $\mathcal{H}_a$ any point whose distance to $\mathcal{H}_a$ is less than $\rho(T)$.

For a point $x \in \trop^d$, its distance to $\mathcal{H}_a$ is
\begin{align}
d_H(x, \mathcal{H}_a) = \max_i(x_i - a_i) - \max_2(x_i - a_i),
\end{align}
where $\max_2$ denotes the second-largest value.

For each point $x_j \in X^1 \cup X^2$, we define a perturbed point $\tilde{x}_j$ as follows:
\begin{itemize}
\item If $d_H(x_j, \mathcal{H}_a) \geq \rho(T)$, then $\tilde{x}_j = x_j$ (no perturbation)
\item If $d_H(x_j, \mathcal{H}_a) < \rho(T)$, let $s = \arg\max_i(x_{ji} - a_i)$ be the sector of $x_j$. We set:
  \begin{align}
  \tilde{x}_{ji} = \begin{cases}
  x_{ji} - d_H(x_j, \mathcal{H}_a) & \text{if } i = s \\
  x_{ji} & \text{otherwise}
  \end{cases}
  \end{align}
\end{itemize}

This projection ensures that $\tilde{x}_j$ lies exactly on $\mathcal{H}_a$.

Therefore, for any $x^{c}$ in the tropical convex hull of class $c\in\{1,2\}$,
\begin{equation}
[x^{c}]_{i}\le T^{c}(x^{c})_{i}=\underbrace{\left(T^{c}(x^{c})-T^{c}(a)\right)_{i}}_{\le\,\max_{k\ne i}(x_{k}^{c}-a_{k})}+T^{c}(a)_{i}.\label{eq:major}
\end{equation}
Fix a coordinate $i$. If $x\in X^{1}\cup X^{2}$ is in sector $s\ne i$, and reaches it second argmax at coordinate $t$, then for $k\ne s$, by definition,
$(\tilde{x}_{j}-a)_{k}\le(x_{j}-a)_{t}\le(\tilde{x}_{j}-a)_{s}$,
hence $\tilde{x}_{jk}-\max_{k\ne i}(\tilde{x}_{jk}-a_{k})\le a_{i}$.
Otherwise, $x_{j}$ is in the $i$-th sector and $\max_{k\ne i}(\tilde{x}_{jk}-a_{k})=x_{jt}-a_{t}$,
thus
\[
\tilde{x}_{ji}-\max_{k\ne i}(\tilde{x}_{j}-a)_{k}=(\tilde{x}_{j}-a)_{i}-(\tilde{x}_{j}-a)_{t}+a_{i}\ge a_{i},
\]
with equality if $d_{H}\left(x_{j},\mathcal{H}_{a}\right)\le\rho(T)$.

Suppose by symmetry that $T(a)_{i}=T^{1}(a)_{i}=\rho(T)+a_{i}$. We
also have $T^{2}(a)\ge\rho(T)+a_{i}$. Then, using the proof of Theorem
22 in \cite{akian2020}, there exists witness points $x_{j_{1}}\in X^{1}$ and
$x_{j_{2}}\in X^{2}$ in sector $i$ , with $x_{j_{1}}$ being at
distance $\rho(T)$ from $\mathcal{H}_{a}$ and $x_{j_{2}}$ at distance
greater than $\rho(T)$. Therefore, $\tilde{x}_{j_{1}i}-\max_{k\ne i}(\tilde{x}_{j_{1}k}-a_{k})=a_{i}$
and $\tilde{x}_{j_{2}i}-\max_{k\ne i}(\tilde{x}_{j_{2}k}-a_{k})\ge a_{i}$.
Moreover, for any $j$ such that $x_{j}\in X^1$ is in sector $i$,
Equation \ref{eq:major} gives $d_{H}(x_{j},\mathcal{H}_{a})\le\rho(T)$.

Let $\tilde{T}^{1}$ and $\tilde{T}^{2}$ be the diagonal-free projections
over transformed projections, and $\tilde{T}=\min(\tilde{T}^{1},\tilde{T}^{2})$.
We have just shown that $\tilde{T}(a)_{i}=\tilde{T}^{1}(a)_{i}=a_{i}$,
and finally $\tilde{T}(a)=a$.
\end{proof}

We summarize the proof of the Main Theorem:

\begin{proof}[Proof of Theorem~\ref{thm:spectral_separability}]
For part 1 (Separability Criterion): If the data are separable with margin $\gamma > 0$, then by Lemma~\ref{lemma:hyperplane_to_operator}, $\rho(T) \leq -\gamma < 0$. Conversely, if $\rho(T) < 0$, then by Lemma~\ref{lemma:operator_to_hyperplane}, the data are separable.

For part 2 (Margin Optimality): Lemma~\ref{lemma:hyperplane_to_operator} shows that no hyperplane can achieve a margin larger than $-\rho(T)$, while Lemma~\ref{lemma:operator_to_hyperplane} provides a construction achieving exactly this margin.

For part 3 (Soft-Margin Interpretation): Lemma~\ref{lemma:perturbation} shows that in the binary case, $\rho(T)$ is positive and characterizes the overlap between the tropical convex hulls.
\end{proof}

\section{Empirical Evaluation}\label{appendix:empirical}

To validate our theoretical complexity analysis, we evaluated both standard tropical SVM and an enhanced tropical polynomial implementation (with $k=4$ sampling points per class) against scikit-learn's LinearSVC on benchmark datasets \cite{scikit-learn,waveform_database_generator_(version_1)_107}. All experiments used 5-fold cross-validation with standardized features. We compare training times for a fixed convergence threshold. All experiments were conducted on a MacBook Air M2.

\begin{table}[h]
\centering
\footnotesize
\begin{tabular}{@{}l@{\hskip 4pt}c@{\hskip 4pt}c@{\hskip 8pt}cc@{\hskip 8pt}cc@{\hskip 8pt}cc@{}}
\toprule
\multirow{3}{*}{\textbf{Dataset}} & \multirow{3}{*}{\textbf{\#C}} & \multirow{3}{*}{\textbf{\#S}} & \multicolumn{6}{c}{\textbf{Accuracy (\%) / Training Time (s)}} \\
\cmidrule(lr){4-9}
& & & \multicolumn{2}{c}{\textbf{Tropical SVM}} & \multicolumn{2}{c}{\textbf{Tropical Poly}} & \multicolumn{2}{c}{\textbf{Linear SVC}} \\
\cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
& & & Acc & Time & Acc & Time & Acc & Time \\
\midrule
Iris & 3 & 150 & 66.7 & 0.0004 & 93.3 & 0.038 & 92.7 & 0.0003 \\
Wine & 3 & 178 & 73.7 & 0.0008 & 92.7 & 0.036 & 97.8 & 0.0004 \\
Breast Cancer & 2 & 569 & 82.6 & 0.0025 & 91.7 & 0.174 & 96.7 & 0.0006 \\
Waveform & 3 & 5000 & 50.5 & 0.021 & 63.7 & 3.571 & 86.7 & 0.0147 \\
\bottomrule
\end{tabular}
\vspace{0.5em}
\caption{Performance comparison across benchmark datasets (5-fold cross-validation). Accuracy standard deviations range from 1.9\% to 9.6\%. \#C: number of classes; \#S: number of samples.}
\label{tab:benchmark_results}
\end{table}

The standard tropical SVM implementation exhibits training times close to LinearSVC on smaller datasets, demonstrating practical computational feasibility. The tropical polynomial variant requires additional computation but substantially improves accuracy, approaching LinearSVC on datasets like Iris.

Notably, the spectral radius values closely align with our theoretical predictions: datasets yielding negative spectral radius values (Wine: -7.53, Breast Cancer: -1.76, Iris: -0.97) exhibited the highest tropical polynomial accuracy, while the non-separable Waveform dataset (spectral radius: 0.87) proved more challenging.

As expected, the number of monomials increases from $d+1$ in standard tropical SVM to $O(d^k)$ in the polynomial variant, explaining the observed computation-accuracy tradeoff. While specialized classical SVM implementations remain faster, our results conclusively demonstrate that tropical SVMs can be practically computed with pseudo-polynomial guarantees.

\begin{figure}[htbp]
    \centering
    % First subfigure
    \begin{subfigure}{0.48\textwidth}
        \centering
        \resizebox{0.95\textwidth}{!}{\input{figures/pca_degree_scaling.pgf}}
        \caption{Training time vs. number of monomials}
        \label{fig:pca_degree_scaling}
    \end{subfigure}
    \hfill  % Adds horizontal spacing between subfigures
    % Second subfigure
    \begin{subfigure}{0.48\textwidth}
        \centering
        \resizebox{0.95\textwidth}{!}{\input{figures/sample_size_scaling.pgf}}
        \caption{Training time vs. size of training set}
        \label{fig:sample_size_scaling}
    \end{subfigure}
    \caption{Performance scaling analysis of tropical SVM on MNIST data}
    \label{fig:scaling_analysis}
\end{figure}

Figure~\ref{fig:scaling_analysis} reveals that the tropical SVM algorithm exhibits linear scaling behavior both with respect to the number of effective monomials and the training set size. We used a Python script that systematically varies PCA dimensions, polynomial degrees, and sample sizes on MNIST data \cite{MNIST}.

\newpage
\section*{NeurIPS Paper Checklist}

\begin{enumerate}

\item {\bf Claims}
    \item[] Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
    \item[] Answer: \answerYes{}
    \item[] Justification: The abstract and introduction clearly state the contributions—including the spectral characterization, margin optimality, pseudo‐polynomial algorithm, and extension to tropical polynomials—which are supported by both theoretical results and empirical evaluations.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the abstract and introduction do not include the claims made in the paper.
        \item The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. 
        \item The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. 
        \item It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 
    \end{itemize}

\item {\bf Limitations}
    \item[] Question: Does the paper discuss the limitations of the work performed by the authors?
    \item[] Answer: \answerYes{}
    \item[] Justification: The paper explicitly discusses limitations such as sensitivity to outliers and the challenge of incorporating soft margins (see the ``Limitations'' paragraph in Section~\ref{sec:spectral})
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. 
        \item The authors are encouraged to create a separate "Limitations" section in their paper.
        \item The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
        \item The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
        \item The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
        \item The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
        \item If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
        \item While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
    \end{itemize}

\item {\bf Theory Assumptions and Proofs}
    \item[] Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?
    \item[] Answer: \answerYes{}
    \item[] Justification: All major theoretical results—including the spectral separability theorem—are supported by clearly stated assumptions and complete proofs provided in the Appendix~\ref{appendix:proofs}.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include theoretical results. 
        \item All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
        \item All assumptions should be clearly stated or referenced in the statement of any theorems.
        \item The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. 
        \item Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
        \item Theorems and Lemmas that the proof relies upon should be properly referenced. 
    \end{itemize}

    \item {\bf Experimental Result Reproducibility}
    \item[] Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?
    \item[] Answer: \answerYes{}
    \item[] Justification: The algorithm is fully described in Section~\ref{sec:algorithm}. The experimental section provides details on data splits (5-fold cross-validation), training times, accuracy metrics, and references a publicly available GitHub repository for code and reproduction (see Table~\ref{tab:benchmark_results} and Appendix~\ref{appendix:empirical}).
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include experiments.
        \item If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
        \item If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. 
        \item Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
        \item While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example
        \begin{enumerate}
            \item If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.
            \item If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.
            \item If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).
            \item We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
        \end{enumerate}
    \end{itemize}


\item {\bf Open access to data and code}
    \item[] Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?
    \item[] Answer: \answerYes{}
    \item[] Justification: The paper includes a link to a GitHub repository (\url{https://github.com/samuelbx/tropical-svm}) with code and instructions for reproducing the experiments.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that paper does not include experiments requiring code.
        \item Please see the NeurIPS code and data submission guidelines (\url{https://nips.cc/public/guides/CodeSubmissionPolicy}) for more details.
        \item While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
        \item The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (\url{https://nips.cc/public/guides/CodeSubmissionPolicy}) for more details.
        \item The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
        \item The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
        \item At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
        \item Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
    \end{itemize}


\item {\bf Experimental Setting/Details}
    \item[] Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?
    \item[] Answer: \answerYes{}
    \item[] Justification: The paper details the experimental setup, including the use of 5-fold cross-validation, data standardization, and provides training times and accuracy metrics in Table~\ref{tab:benchmark_results} and related figures in the appendix.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include experiments.
        \item The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
        \item The full details can be provided either with the code, in appendix, or as supplemental material.
    \end{itemize}

\item {\bf Experiment Statistical Significance}
    \item[] Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?
    \item[] Answer: \answerYes{}
    \item[] Justification: The results are reported with standard deviations.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include experiments.
        \item The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
        \item The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
        \item The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
        \item The assumptions made should be given (e.g., Normally distributed errors).
        \item It should be clear whether the error bar is the standard deviation or the standard error of the mean.
        \item It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96\% CI, if the hypothesis of Normality of errors is not verified.
        \item For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
        \item If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
    \end{itemize}

\item {\bf Experiments Compute Resources}
    \item[] Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?
    \item[] Answer: \answerYes{}
    \item[] Justification: All experiments were conducted on a MacBook Air M2. These specifications are typical for small-scale benchmark experiments, and the reported training times reflect the performance on this hardware.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include experiments.
        \item The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
        \item The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. 
        \item The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). 
    \end{itemize}
    
\item {\bf Code Of Ethics}
    \item[] Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics \url{https://neurips.cc/public/EthicsGuidelines}?
    \item[] Answer: \answerYes{}
    \item[] Justification: The work is theoretical and algorithmic with supporting experiments on standard benchmark datasets; there are no ethical issues or concerns related to the research.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
        \item If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
        \item The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
    \end{itemize}


\item {\bf Broader Impacts}
    \item[] Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?
    \item[] Answer: \answerNA{}
    \item[] Justification: The research is primarily theoretical and methodological with no direct societal impact, and no discussion of broader impacts is necessary.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that there is no societal impact of the work performed.
        \item If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
        \item Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
        \item The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
        \item The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
        \item If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
    \end{itemize}
    
\item {\bf Safeguards}
    \item[] Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?
    \item[] Answer: \answerNA{}
    \item[] Justification: The work does not involve the release of data or models that present a high risk of misuse.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper poses no such risks.
        \item Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. 
        \item Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
        \item We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
    \end{itemize}

\item {\bf Licenses for existing assets}
    \item[] Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?
    \item[] Answer: \answerYes{}.
    \item[] Justification: All referenced works and datasets are properly cited, and the GitHub repository is expected to include license information consistent with proper academic practice.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not use existing assets.
        \item The authors should cite the original paper that produced the code package or dataset.
        \item The authors should state which version of the asset is used and, if possible, include a URL.
        \item The name of the license (e.g., CC-BY 4.0) should be included for each asset.
        \item For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
        \item If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, \url{paperswithcode.com/datasets} has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
        \item For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
        \item If this information is not available online, the authors are encouraged to reach out to the asset's creators.
    \end{itemize}

\item {\bf New Assets}
    \item[] Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?
    \item[] Answer: \answerYes{}
    \item[] Justification: The paper introduces a novel algorithm and provides experimental code, with documentation available in the supplementary GitHub repository.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not release new assets.
        \item Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. 
        \item The paper should discuss whether and how consent was obtained from people whose asset is used.
        \item At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
    \end{itemize}

\item {\bf Crowdsourcing and Research with Human Subjects}
    \item[] Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? 
    \item[] Answer: \answerNA{}
    \item[] Justification: The work does not involve crowdsourcing or human subjects.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
        \item Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. 
        \item According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 
    \end{itemize}

\item {\bf Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects}
    \item[] Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?
    \item[] Answer: \answerNA{}
    \item[] Justification: The research does not involve human subjects, so IRB approval is not applicable.
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
        \item Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. 
        \item We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. 
        \item For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.
    \end{itemize}

\end{enumerate}


\end{document}
